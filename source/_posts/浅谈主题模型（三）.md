---
title: 浅谈主题模型（三）—— LDA模型简介
date: 2017-03-28 23:00:27
tags: [NLP,LDA,主题模型,贝叶斯]
categories: 
- Machine Learning
- NLP
- Topic Model
---

承接“浅谈主题模型（二）”中的LDA的概率知识，本文着重介绍LDA模型。
<!-- more -->

## LDA Model
LDA模型的本质是一个unigram模型，模型中每个词的生成都与上下文无关。unigram模型中假设文本中的词服从multinomial分布，同时multinomial分布的先验分布为Dirichlet分布。

LDA 模型作为一个关于语料库的概率发生模型。它的基本思想是文档被表示为基于潜在主题的随机混合，其中每个主题由于基于词的分布不同而与众不同。

通过下面这张贝叶斯图，我们可以更好地理解LDA模型

![model 1](/images/old-resources/model 1.png)

图中的框表示可重复执行的器件。外框表示的是文档，而内框表示对一篇文档内的主题和词的重复选择。

对于LDA模型，我们有如下定义：

1. 词语是离散数据中的基本单元，我们将其定义为{1, ..., V}词库中的一员。我们通过这些基本单元的向量来表示词，在这些向量中，都只有一个元素为 1，其余都是 0。因此，我们采用上标字符来表示向量中的元素，词库中的第 v 个词语表示为一个长度为 V的向量w，向量中$w_v = 1，w_u = 0$，其中u ≠ v。
2. 文档是一序列N个词，表示为$w = (w_1, w_2, ..., w_N)$，其中$w_N$是这一序列中的第N个词
3. 语料库是一个M个文档的集合，表示为$D = {\mathbf{w_1}, \mathbf{w_2}, ..., \mathbf{w_M}}$
4. z表示为主题，主题数为K


LDA为语料库D中的每个文档w假定以下生成过程

1. $\alpha , \beta$ 分布为Dirichlet分布的先验参数，$\theta \sim Dir(\alpha), \phi \sim Dir(\beta)$
2. $\theta$为文档-主题的大学生分布，$\phi$为主题-词的多项式分布，其中的主题数为K
3. 对于某一文档d中的一个词w，取主题$z \sim Multi(\theta_d)$， 生成对应词$w \sim Multi(\phi_z)$


在构建模型的过程中，采用的是 Gibbs 采样的方法，通过一直迭代到收敛训练出最终的文档-主题分布θ、主题-词分布φ。

利用 LDA 模型，我们可以获得文档潜在主题的分布，并利用朴素贝叶斯分类器用这些信息实现对相同文档的匹配等。


## Gibbs Sampling

由于LDA中的主题z，文档-主题分布θ、主题-词分布φ都是未知的隐含变量，是需要根据观察到的文档集合中的词来学习估计的，所以不能通过最大似然估计的方法来确定主题分布，我们一般采用Gibbs Sampling的方法通过迭代得到文档-主题分布θ、主题-词分布φ。

Gibbs Sampling 是Markov-Chain Monte Carlo算法的一个特例。这个算法的运行方式是每次选取概率向量的一个维度，给定其他维度的变量值Sample当前维度的值。不断迭代，直到收敛输出待估计的参数。

换句话说，就是每一维度都是由其他维采样生成的，马氏链的转移只是轮流沿着该维度内的单条坐标轴进行转移，无法沿着单条坐标轴转移的，转移概率都设为0.

图示如下：

![](/images/old-resources/14908762507727.jpg)


1. 初始时我们随机给文本中的每个单词分配主题$z^{(0)}$，
2. 然后统计每个主题z下出现term t的数量以及每个文档 m 下出现主题 z 中的词的数量，每一轮计算 $p(z_i|\mathbf{Z}_{-i}, \mathbf{d}, \mathbf{w})$，即排除当前词的主题分配，根据其他所有词的主题分配估计当前词分配各个主题的概率。
3. 当得到当前词属于所有主题z的概率分布后，根据这个概率分布为该词sample一个新的主题$z^{(1)}$。
4. 然后用同样的方法不断更新下一个词的主题，直到发现每个文档下Topic分布$\mathbf{\theta_m}$和每个Topic下词的分布$\mathbf{\phi_k}$收敛，算法停止，输出待估计的参数$\mathbf{\theta_m}$和$\mathbf{\phi_k}$。
5. 最终每个单词的主题也$z_{m,n}$同时得出。实际应用中会设置最大迭代次数。每一次计算$p(z_i|\mathbf{Z}_{-i}, \mathbf{d}, \mathbf{w})$的公式称为Gibbs updating rule。


关于LDA模型通过Gibbs Sampling训练得到参数的具体数学公式推导，可以详见[Distributed Gibbs Sampling of Latent Topic Models: The Gritty Details](https://pdfs.semanticscholar.org/a166/d65a5d5a2905b038288e59c4fd98864c6f44.pdf)


## 应用
训练得到LDA模型后，我们能够用这个模型做什么呢？在这里我简要介绍两种。

### 计算文档的topic语义分布
我们认定，对于一个新的文档$d_{new}$，之前训练阶段得到的主题-词分布φ是不变的，可以通过这样的方法得到该文档中的文档-主题分布$\theta_{new}$

1. 对文档中的每个词随机赋予一个topic z
2. 按照Gibbs Sampling的公司，对每个词重新采样它的topic
3. 重复上述过程直到收敛
4. 统计文档中的topic分布，得到该文档中的文档-主题分布$\theta_{new}$

### 文档匹配
我们可以用朴素贝叶斯分类器为新输入的文档找到与之前文档中最匹配的。

公式如下：

$$
P(d|d_{new}) \propto P(d) P(d_{new} | d) = P(d) \prod_{w \in d_{new}} \sum_z \phi_{w|z} \theta_{z|d}
$$

这里z表示topic。用这个贝叶斯公式，我们通过遍历所有的训练集，找到匹配概率最大的文档就是语义最相似的文档。

下一节我会简要介绍下我基于LDA模型的毕设和省创项目



