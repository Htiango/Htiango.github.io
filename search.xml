<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[再见多说，你好Disqus]]></title>
      <url>%2F2017%2F05%2F29%2F%E5%86%8D%E8%A7%81%E5%A4%9A%E8%AF%B4%EF%BC%8C%E4%BD%A0%E5%A5%BDDisqus%2F</url>
      <content type="text"><![CDATA[早在三个月前就知晓了多说将在6月1日后关闭的噩耗，然而懒癌晚期患者不到最后一刻是不会做出行动的。花了半小时的时间完成了Disqus的搭建，相信这款墙外产品一定会有着比多说更好的体验（前提是梯子要稳。。。） 由于我的hexo选用的主题的next，内置config就支持Disqus的使用，所以我们只需要在Disqus上注册生成一个site(通过首页上点击get started进入)，然后将shortname输入config文件中即可，如下： 1234disqus: enable: true shortname: your-shortname count: true Disqus的相关注册问题可以参考这篇文章 配置完成后，只要更新hexo即可启用Disqus评论，enjoy it！ （至于如何将原有的评论从多说导出到Disqus，github上的这篇文章已经说的很详细，按步骤做就行，很方便的）。 最后再说一遍，国内用梯子一定要稳，一定要稳！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Maven的配置(Mac)]]></title>
      <url>%2F2017%2F05%2F28%2F%E9%85%8D%E7%BD%AEMaven%2F</url>
      <content type="text"><![CDATA[最近在公司负责一款避孕智能咨询的聊天机器人，是嵌入手淘中的。上周刚刚基于Lucene为机器人添加了搜索引擎，在项目进行的过程中深深领略到了maven的过人之处。 maven作为一款Java项目的管理工具，在项目依赖管理上的优点简直完美！只需在pom中添加一小段描述以及配置信息，就可以在项目里调用外包中的函数，岂不美哉，下面简要介绍下Maven的配置和使用。 首先从官网下载最新的maven的binary并进行解压。然后我们需要确定在Mac上已经设置了JAVA_HOME并指向了你的jdk安装位置，没有的话在Terminal中进行如下操作： 1$ vi ~/.bash_profile 添加： 12345export JAVA_8_HOME=your_java8_homeexport JAVA_7_HOME=your_java7_homeexport JAVA_HOME=$JAVA_8_HOMEexport PATH=$&#123;PATH&#125;:$&#123;JAVA_HOME&#125;/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 此时我们已经添加了JAVA_HOME，现在需要安装maven，同样也是在该文件下进行添加 12export MAVEN_HOME=your_maven_download_direxport PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/bin 退出后更新 1$ source ~/.bash_profile 这时输入mvn -v应该可以看到Maven的相关信息 安装完成！ 至于maven的使用，墙裂推荐使用JetBrains系列下的IDEA，反正对教育邮箱都是免费的，很赞。我接下来会通过一个简单的项目介绍Lucene，届时也会进一步深入maven的使用。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[香港游记（一）—— 暴走HK]]></title>
      <url>%2F2017%2F04%2F11%2F%E9%A6%99%E6%B8%AF%E6%B8%B8%E8%AE%B0%2F</url>
      <content type="text"><![CDATA[游历了不少祖国的大川大山，走过了许多塞外的草原沙丘，我一直想体验一番现代都市的独特魅力。趁着入职前的空档，我和室友抽空去香港转了一番。此行若不以游记的形式展现出来，那就实在是太虚度光阴了，我将分成三篇随笔，简要介绍下此次的香港之行。 匆忙的起始此行的香港之行可以说是一趟说走就走的旅程。之前一直嘴上喊着拿到实习offer就立马开赴香港，然而当offer真正到来的时候我也是着实犹豫了很久才最终下定了赴港的决心。多亏了璠哥一个月前的经验，我几乎没用花太多的时间准备行程，比较轻松地就定下了四天后前往香港的旅程。 虽是定的周日启程，但是上午还得参加组会。尽管我早就和导师言及了去实习的事，但是内心还是非常挣扎的想要站好最后一班岗，周日的组会时间总是这么的尴尬，这次也不例外，启程前的组会变得如此的漫长。 熬到了组会结束，我立马飞奔回寝室整好行李后直接开赴广州东站，坐上了开往hk的绿皮列车广九线。不得不说广九线入港比从深圳入关方便许多，既不用排很长的队伍，又可以直接在闹市区下站，节省了大把的时间。 红磡下车，绕着车站没边际地走了一圈，才在百度地图的指引和对面红砖砌成的港理工的参照下找回了方位感。为了更好地体验香港，我们决定开着11路直奔维港的天星小轮码头，在沿途我还特意找了执念许久的第三代肥仔小食店，品尝了大生肠、火鸡肾和大墨鱼，简直美味！独特的口味深受喜欢猎奇的我的喜爱。而这种酒香巷深的既视感也充满了独特的市井气息。 然后又左拐右拐穿过一个又一个小巷，去了池记吃到了美味的蟹黄粥，赶在日落前走到维港坐上了天星小轮，横渡到了中环。然后又以惊人的体力开着11路走了2公里多抵达酒店。这还不算完，放下行李稍作歇息，我和室友又马不停蹄从港大后门绕道上了太平山。 夜上太平山夜晚的太平山上山小道格外静谧，几乎看不到一个人影，只有不时乱做的风吹打着道路两边的灌木，与昏暗的灯光一起构成了诡异的氛围。想必是之前听到些太平山的奇闻怪事吧，此时此刻我竟有些慌乱，额头有些渗出冷汗。是夜的太平山云雾缭绕，行及山腰就一片朦胧，水气弥漫了。据说站在太平山顶眺望香港城，可以切身感受到什么叫做繁华都市，什么叫做摩登时代。做攻略时我也对不少星光熠熠的美景图垂涎三尺。可惜这一切都随着雾气的弥漫消失殆尽，虽然山顶的大风源源不断地将雾气推向一座座摩天大楼，但是身后的浓雾马上又马不停蹄地接了上来，只有短短的几瞬才能让我拍出几张朦胧之美的夜香港。无奈的我只能和室友调侃到看来是要用去雾算法处理图像了哈哈。或许人生就应该是这样带有遗憾的吧，挺好，给我下次来香港增添一些憧憬吧。 不同于印象中的游记写的那样人流众多，此时的山顶不过寥寥几人，或许是10点多的关系，又或许是这不识时务的云雾所致吧。下山的缆车竟然出人意料地不用排队，我们顺顺当当的坐上了下山缆车，又开启了11路走到了兰桂坊，打算替璠哥完成未能逛夜店的遗憾。还未走近我们就听到了阵阵嘈杂的喧哗声，一群群拿着酒杯的外国人堆叠在一个个酒吧的门口，有又唱又跳的，有左拥右抱的，还有激情谈论的，我们深感与此处的格格不入，又实在不忍心就这样弃夜香港于不顾，只好在周招找了一个相对安静点的正放着球赛的酒吧，点了几杯啤酒边看球赛边评头论足。 或许这种热闹的氛围会传染吧，一对对身着奇装异服的外国人在店门口驻足停留，我忍不住走上前去要求合影，于是就有了这样的囧样 清吧的酒或许终究没有劲吧来的好喝吧，手中的啤酒越喝越苦，人也越来越困，忙不迭的干完剩余的酒踏上了回宾馆的路。 如果是这就是我们香港之旅的第一天的话，那可真的是够疯狂的。可是疯狂的事情还没结束呢，才刚刚走出兰桂坊，我就瞅见了翠华那大大的招牌，心想那敢情好啊，送上门的美食哪能不吃？可是嘴上却说太晚了不吃了要忍住。要是我真的是能忍住的动物就好了，走了两三步毅然回头冲入酒店开吃，一碗虾子云吞面一份猪扒包甚慰我心我胃。 酒足饭饱早已离地铁关门时间过去了一个小时，心想反正也不差这点步数了，就又开了好久的11路才回到宾馆，此时早已过了凌晨两点好久了，身心俱疲，不过内心那种见证过夜香港的自豪根本压抑不住。what a day！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[语音识别简介（一）——特征提取]]></title>
      <url>%2F2017%2F04%2F03%2F%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%AE%80%E4%BB%8B%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
      <content type="text"><![CDATA[上学期，我们学习了 “Design and Implementation of Speech Recognition System” 这门课 （CMU 18799）。通过这门课的学习，我们了解语音识别的基础知识并最终实现了连续语音的识别（采用HMM-GMM模型）。本文主要介绍下进行语音识别前的一些预处理工作，即是如何提取出特征的。 就提取特征的预处理而言，具体可以分为这几个步骤：采样、分帧加窗、endpointing、MFCC、DCT到一个确定的维度。 Sampling 采样（录制）现实中的声音属于一种模拟信号，要想让计算机能够对声音进行处理，我们首先要做的就是将模拟信号转换为数字信号。那么我们又该如何用计算机实现这个采样的过程呢？在具体操作的过程中，我们使用的是 PortAudio 开源库来录制声音。 我当时是在Mac OS系统下用Xcode配置了PortAudio，具体的配置过程可以参见这篇博文，在此不作赘述。 于是通过PortAudio这个库我们可以实现语音的录制，即采样。得到的文件中的数值参见具体的文件后缀格式而定。接下来我们需要对语音进行分帧处理 分帧加窗为了更好地去除噪声的影响，我们需要加一个preemphasize 的过程：即 $s[n] = s[n] - \alpha * s[n - 1]，\alpha$一般接近1. 在Mac OS系统下，一般采用的采样频率是44100hz，每个点代表的时间约为0.02ms，对于声音信号而言，这个范围太小了，既没有区间差异性又会导致计算量的增多，所以我们这里将20ms这一时间段视为一个特征点，这就需要我们进行分帧处理，一般采用移动窗函数来实现，帧与帧之间有overlap，这里我们将overlap定为10ms，之所以要有overlap是为了更好地描述上一帧中被忽略的部分，防止信息的丢失。 由于有了overlap，所以如果用rectangular 窗函数进行分帧的话会导致不平滑的现象，在这里我们采用hanning窗函数进行分帧，可以得到平滑的帧。同时加窗函数的意义还在于可以比截断部分刚好地体现出原始信号的频率信息： 最终得到的是就是一帧帧的信号（可以理解为一个二维数组）。 Endpointing对于一串语音而言，其中有很大的一部分是没有声音的，尤其是以开头和结尾处为主。较多的non-speech部分不仅会导致不必要的计算负担，同时还可能会将non-speech的部分错误地识别成一些词。 为了防止出现这种现象，我们首先要找出speech region，即做一个endpointing： 如何endpointing呢？我们基于的原理就是speech region较之non-speech region有着较高的能量。一种最简单的方法就是直接对信号施加一个阈值，当能量超过阈值的时候，其对应的就很有可能是speech region。当然这种方法忽略了背景噪音等一些因素，因此往往不能准确对speech region进行划分。下面介绍两种更加准确的endpointing的方法： 第一种也叫做 Adaptive Endpointing Algorithm，其核心思想就是用一个smoothed signal level 和一个fast-varying estimation of background进行比较，之所以被称为adaptive就是指adapt background level。 伪代码见下图： 另一种更复杂一点的方法就是对speech部分的开始和结尾分别处理：开始部分会有一个很明显的能力增加的过程，即便是在高噪声的环境中仍然如此；而结束的部分由于延长音等一些因素，使得能量的降低不是那么明显，尤其是在高噪声的环境中更是如此，所以我们采用两个阈值分别处理语音的开始部分和结尾部分。伪代码见下： MFCC经过上述处理后，我们得到了一个包含着较少non-speech region的一帧帧信息，然而这里的信息都只是在时域上的，不能反映出什么，我们需要对其进行傅里叶变换，将其转为频域上的信息。 这里为了方便进行fft快速傅里叶变换，我们需要先进行zero padding的过程，关于zero-padding可以参见我之前的一篇博文 将其转到频域之后，我们好像需要将频率匹配到是和人耳听觉的区域（即mel域） mel域可以认为是对频域做一个warp的过程，每处的warp不同。这主要是由于人耳对低频变化比较敏感而对高频的不敏感。频域到mel域的公式为： $$z = 1125 \ln(1 + \frac{f}{700})$$ 下图中横轴是原始频域，纵轴是mel域。 实际操作时我们一般用一定数量的filter bank得到对应数目的mel域的值（一般采用三角滤波）（one value per filter） 为了对数据进行更好地压缩，我们再对求得的mel spectrum做一个log变换（同时也是为了reduce imbalance） DCT变换得到了log mel spectrum的值后，每一帧的维度可以认为是filter的数目，为了方便计算我们还需要对其进行降维处理，这里采用的是DCT变换将维度变为13维。在DCT中减去的维度在不同声音间有着较小的变化。 在得到13维的特征向量之后，我们还想进一步得到一些关于频率变化的信息，于是我们对13维的每一维分别求一阶导和二阶导（由于一帧的时间很短，可以认为是$\Delta t$）最终得到的特征向量有39维度 通过上述这些方法，我们就得到了进行语音识别的特征点。 在下一节中我将介绍如何用DTW to HMM的方法进行简单的语音识别。 项目代码这一部分的相关代码可以详见我的github中的 SpeechRecognition项目]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[浅谈主题模型（五）—— Reference]]></title>
      <url>%2F2017%2F03%2F28%2F%E6%B5%85%E8%B0%88%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%BA%94%EF%BC%89%2F</url>
      <content type="text"><![CDATA[承接“浅谈主题模型（四）”中的项目模型的介绍，本文主要展示下项目相关代码以及Reference。 ReferenceLDA模型作为我毕设和省创的一个base，花了我相当多的时间来学习和理解，期间也查阅了许多的文献和博文，最终的代码实现也是一个坑一个坑踩出来的。 所以我在此将有用的资料一一列举出来，同时也将自己所有的代码开源化，希望大家能少走些弯路。 项目的相关代码详见我Github中的Advanced-LDA-Program LDA的相关代码详见我Github中的Chinese-LDA-v1.0 如果想学习FudanNLP的中文分词的使用，可以参见我Github中的FudanNLP-Example Hofmann T. Probabilistic latent semantic indexing[C]//Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 1999: 50-57. Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. the Journal of machine Learning research, 2003, 3: 993-1022. Zhang K, Wu W, Wu H, et al. Question retrieval with high quality answers in community question answering[C]//Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. ACM, 2014: 371-380. Yang L, Qiu M, Gottipati S, et al. Cqarank: jointly model topics and expertise in community question answering[C]//Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management. ACM, 2013: 99-108. 概率语言模型及其变形系列 LDA数学八卦 从贝叶斯方法谈到贝叶斯网络 LDA话题模型与推荐系统]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[浅谈主题模型（四）—— 项目简介]]></title>
      <url>%2F2017%2F03%2F28%2F%E6%B5%85%E8%B0%88%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
      <content type="text"><![CDATA[承接“浅谈主题模型（三）”中的LDA模型的介绍，本文主要介绍我基于LDA模型的毕设和省创项目——UQA模型。 项目背景在当今的互联网时代，医疗服务不再仅仅是停留在线下的医院诊所里，我们还可以通过互联网获取各式各样的医疗服务，而这其中以网络社区的问答系统最为流行。 为了充分发挥网络问答社区的优势，我本次毕设项目的研究目的就是实现网络问答社区的智能推荐。通过问答社区中的海量问答信息，利用主题模型进行分析处理，实现对新输入问题的最佳答案以及最佳专家的智能推荐。不仅仅保证了推荐专家和推荐答案的准确性，同时还保证用户能够在第一时间内获取答案，不必再耗费大量的时间等待专家来进行回答。 UQA模型简介在 LDA 模型中，我们将文档视为一系列主题的分布，通过文档-主题以及主题-词这两个多项式分布实现了对文档中潜在主题的挖掘与分析。然而，结合我们的实验数据 (用户提问)，我们得出结论:专家进行问题回答时是有一定的偏向性的，每位专家有着他们各自偏好的东西，不仅仅是他们擅长的话题领域，还可以包括用户的提问方式等等。因此，我们假定除了主题之外，文档 D 中还有着吸引着专家对用户提问进行回答的东西，在本文中我们将其称之为专家专长(expertise)。因此，我们将专家和专家专长这两个概念引入原有的 LDA 模型中，构建一个基于 LDA 模型的 UQA(USER- QUESTION-ANSWER)模型，利用这个模型，我们不仅仅可以实现对新问题的最佳答案智能推荐，同时还可以实现对新问题的最佳专家推荐。 如下图所示的 UQA 模型中，我们在 LDA 的基础上补充如下定义: 专家集U是所有回答专家的集合，表示为$U = {u_1, u_2, … , u_n}$ x表示专家转成，专长数为E个 UQA模型为语料库D中的每个文档 w 假定以下生成过程: $\alpha, \beta, \gamma$分布为Dirichlet分布的先验参数： $\theta \sim Dir(\alpha) , \phi \sim Dir(\beta) , \psi \sim Dir(\gamma)$ θ为文档-主题的多项式分布，ψ为文档-专长的多项式分布，φ为主题/专长-词的联合多项式分布。 对于某一文档 d 中的词 w，取主题 $z \sim Multi(θ_d) $，专长$ x \sim Multi(\psi_d)$ ，生成对应词 $w~Multi(\phi_{z,x})$ （联合分布） UQA模型中的文档-主题的多项式分布θ，文档-专长的多项式分布ψ，主题/专长-词的联合多项式分布φ都是通过Gibbs Sampling的方法得到，推导过程在论文中有详述。在这里只给出伪代码： 1234567891011121314151617181920212223242526272829303132333435// zero all count variable NWZX, NZM, NXU, NS// Initializeforeach document m ∈ [1,D] do get the author u of the this document m; foreach word n ∈ [1, Nm] in document m do sample topic index zm,n ~ Mult(1/K) for word wm,n; sample expertise index xu,e ~ Mult(1/X) for word wm,n; // Here m also refers to the author uincrement doc-topic count: NZM[zm,n][m]++;increment user-expertise count: NXU[xu,e][u]++;increment [topic,expertise]-word count: NWZX[wm,n][zm,n][xu,e]++;increment [topic,expertise]-word sum: NS[zm,n][xu,e]++; endend// Gibbs Samplingwhile not finished do foreach document m ∈ [1,D] do get the author u of the this document m; foreach word n ∈ [1, Nm] in document m do NZM[zm,n][m]--; NXU[xu,e][u]--; NWZX[wm,n][zm,n][xu,e]--; NS[zm,n][xu,e]--; Sample topic and expertise index [z’m,n][x’u,e] according to Equ.32 // Because we have minused 1, so in Euq.32 we don’t need to -1 NZM[z’m,n][m]++; NXU[x’u,e][u]++; NWZX[wm,n][z’m,n][x’u,e]++; NS[z’m,n][x’u,e]++; end if converged and L sampling iterations since last read out then read out parameter set Θ, Φ, Ψ according to Equ.33, 34 and 35 in 2.2.2.3; end endend 利用 UQA 模型，我们可以获得文档潜在主题的分布与潜在专家专长分布，并利用这些信息实现对新问题的最佳答案智能推荐以及最佳专家的推荐。 最终也是用朴素贝叶斯找到训练集中与新输入问题最匹配的问题，其对应的答案即是最佳推荐答案 $$\begin{eqnarray} \nonumberP(d|d_{new}) \propto P(d) P(d_{new} | d) &amp; = &amp; P(d) \prod_{w \in d_{new}} P(w | d) \\ \nonumber&amp; = &amp; P(d) \prod_{w \in d_{new}} \sum_z \phi_{w|z} \theta_{z|d} \\ \nonumber&amp; = &amp; P(d) \prod_{w \in d_{new}} \sum_z (\sum_x \phi_{w|z,x}) \theta_{z|d} \\ \nonumber\end{eqnarray}$$ 通过上述这两个公式，我们可以获得训练集中一条提问在新问题下的匹配概率，通过遍历获取最大匹配概率，可以从训练集中获取最相似的问题，该最相似问题的答案即推荐答案。 对于 UQA 模型，我们可以利用贝叶斯定理得到新输入文档$d_{new}$与训练集中的 某一位专家 u 的匹配概率$P(u|d_{new})$(即在新文档发生的情况下，训练集中专家 u 的可能性)，具体公式推导见下： $$\begin{eqnarray} \nonumberP(u|d_{new}) \propto P(u) P(d_{new} | u) &amp; = &amp; P(u) \prod_{w \in d_{new}} P(w | u) \\ \nonumber&amp; = &amp; P(u) \prod_{w \in d_{new}} \sum_x \phi_{w,z|x} \psi_{x|u} \\ \nonumber&amp; = &amp; P(u) \prod_{w \in d_{new}} \sum_x (\sum_z \phi_{w|x,z})\psi_{x|u} \\ \nonumber\end{eqnarray}$$ 通过这个公式，我们可以获得训练集中一位专家在该新问题下的匹配概率，通过遍历获取最大匹配概率，可以从训练集答案中获取最匹配的推荐专家。 结果以下通过图表简要介绍下项目结果。 下图是不同词典、不同主题数下，LDA模型得到的推荐答案的正确率： 下图是不同词典、不同主题数、不同专家专长数下，UQA模型得到的推荐答案的正确率： 下图是不同词典、不同主题数、不同专家专长数下，UQA模型得到的推荐专家的正确率： 在下一节我会将项目的reference以及项目代码展示出来。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[浅谈主题模型（三）—— LDA模型简介]]></title>
      <url>%2F2017%2F03%2F28%2F%E6%B5%85%E8%B0%88%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
      <content type="text"><![CDATA[承接“浅谈主题模型（二）”中的LDA的概率知识，本文着重介绍LDA模型。 LDA ModelLDA模型的本质是一个unigram模型，模型中每个词的生成都与上下文无关。unigram模型中假设文本中的词服从multinomial分布，同时multinomial分布的先验分布为Dirichlet分布。 LDA 模型作为一个关于语料库的概率发生模型。它的基本思想是文档被表示为基于潜在主题的随机混合，其中每个主题由于基于词的分布不同而与众不同。 通过下面这张贝叶斯图，我们可以更好地理解LDA模型 图中的框表示可重复执行的器件。外框表示的是文档，而内框表示对一篇文档内的主题和词的重复选择。 对于LDA模型，我们有如下定义： 词语是离散数据中的基本单元，我们将其定义为{1, …, V}词库中的一员。我们通过这些基本单元的向量来表示词，在这些向量中，都只有一个元素为 1，其余都是 0。因此，我们采用上标字符来表示向量中的元素，词库中的第 v 个词语表示为一个长度为 V的向量w，向量中$w_v = 1，w_u = 0$，其中u ≠ v。 文档是一序列N个词，表示为$w = (w_1, w_2, …, w_N)$，其中$w_N$是这一序列中的第N个词 语料库是一个M个文档的集合，表示为$D = {\mathbf{w_1}, \mathbf{w_2}, …, \mathbf{w_M}}$ z表示为主题，主题数为K LDA为语料库D中的每个文档w假定以下生成过程 $\alpha , \beta$ 分布为Dirichlet分布的先验参数，$\theta \sim Dir(\alpha), \phi \sim Dir(\beta)$ $\theta$为文档-主题的大学生分布，$\phi$为主题-词的多项式分布，其中的主题数为K 对于某一文档d中的一个词w，取主题$z \sim Multi(\theta_d)$， 生成对应词$w \sim Multi(\phi_z)$ 在构建模型的过程中，采用的是 Gibbs 采样的方法，通过一直迭代到收敛训练出最终的文档-主题分布θ、主题-词分布φ。 利用 LDA 模型，我们可以获得文档潜在主题的分布，并利用朴素贝叶斯分类器用这些信息实现对相同文档的匹配等。 Gibbs Sampling由于LDA中的主题z，文档-主题分布θ、主题-词分布φ都是未知的隐含变量，是需要根据观察到的文档集合中的词来学习估计的，所以不能通过最大似然估计的方法来确定主题分布，我们一般采用Gibbs Sampling的方法通过迭代得到文档-主题分布θ、主题-词分布φ。 Gibbs Sampling 是Markov-Chain Monte Carlo算法的一个特例。这个算法的运行方式是每次选取概率向量的一个维度，给定其他维度的变量值Sample当前维度的值。不断迭代，直到收敛输出待估计的参数。 换句话说，就是每一维度都是由其他维采样生成的，马氏链的转移只是轮流沿着该维度内的单条坐标轴进行转移，无法沿着单条坐标轴转移的，转移概率都设为0. 图示如下： 初始时我们随机给文本中的每个单词分配主题$z^{(0)}$， 然后统计每个主题z下出现term t的数量以及每个文档 m 下出现主题 z 中的词的数量，每一轮计算 $p(z_i|\mathbf{Z}_{-i}, \mathbf{d}, \mathbf{w})$，即排除当前词的主题分配，根据其他所有词的主题分配估计当前词分配各个主题的概率。 当得到当前词属于所有主题z的概率分布后，根据这个概率分布为该词sample一个新的主题$z^{(1)}$。 然后用同样的方法不断更新下一个词的主题，直到发现每个文档下Topic分布$\mathbf{\theta_m}$和每个Topic下词的分布$\mathbf{\phi_k}$收敛，算法停止，输出待估计的参数$\mathbf{\theta_m}$和$\mathbf{\phi_k}$。 最终每个单词的主题也$z_{m,n}$同时得出。实际应用中会设置最大迭代次数。每一次计算$p(z_i|\mathbf{Z}_{-i}, \mathbf{d}, \mathbf{w})$的公式称为Gibbs updating rule。 关于LDA模型通过Gibbs Sampling训练得到参数的具体数学公式推导，可以详见Distributed Gibbs Sampling of Latent Topic Models: The Gritty Details 应用训练得到LDA模型后，我们能够用这个模型做什么呢？在这里我简要介绍两种。 计算文档的topic语义分布我们认定，对于一个新的文档$d_{new}$，之前训练阶段得到的主题-词分布φ是不变的，可以通过这样的方法得到该文档中的文档-主题分布$\theta_{new}$ 对文档中的每个词随机赋予一个topic z 按照Gibbs Sampling的公司，对每个词重新采样它的topic 重复上述过程直到收敛 统计文档中的topic分布，得到该文档中的文档-主题分布$\theta_{new}$ 文档匹配我们可以用朴素贝叶斯分类器为新输入的文档找到与之前文档中最匹配的。 公式如下： $$P(d|d_{new}) \propto P(d) P(d_{new} | d) = P(d) \prod_{w \in d_{new}} \sum_z \phi_{w|z} \theta_{z|d}$$ 这里z表示topic。用这个贝叶斯公式，我们通过遍历所有的训练集，找到匹配概率最大的文档就是语义最相似的文档。 下一节我会简要介绍下我基于LDA模型的毕设和省创项目]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[浅谈主题模型（二）—— 相关概率知识]]></title>
      <url>%2F2017%2F03%2F28%2F%E6%B5%85%E8%B0%88%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
      <content type="text"><![CDATA[承接“浅谈主题模型（一）”中的主题模型的历史，本文着重介绍LDA模型的相关概率知识，为接下来介绍LDA模型打下基础。 在LDA模型中，涉及到相当多的数学尤其是概率统计方面的知识，在具体介绍LDA模型之前，首先对这些概率知识进行一个总结归纳。 在LDA模型框架下，我们认为文本是一系列服从一定概率分布的词项的样本集合：每个文档中词的Topic分布服从Multinomial分布，其先验选取共轭先验即Dirichlet分布；每个Topic下词的分布服从Multinomial分布，其先验也同样选取共轭先验即Dirichlet分布。 接下来主要介绍Dirichlet分布以及其中涉及到的Gamma函数的相关知识。 Gamma 函数Gamma函数：$$\Gamma(x) = \int_{0}^{\infty}{t^{x-1} e^{-t} dt}$$ 通过分部积分的方法，可以推导出这个函数有着如下的递归性质：$$\Gamma(x + 1) = x \Gamma(x)$$ 可以证明得到，$\Gamma(x)$函数可以当成是阶乘在实数集上的延拓，具有如下性质： $$\Gamma(n) = (n - 1)!$$ 下面简要地介绍下gamma函数的一些应用 求函数的分数阶导数Gamma 函数可以用于求分数阶导数： 对于 $x^n$ 的各阶导数： first derivative： $n x^{n - 1}$ second derivative： $n (n - 1) x^{n - 2}$ ··· k-th derivative： $ \frac{n!}{(n - k)!} x^{n - k}$ 由于k阶导数可以用阶乘表示，于是我们用Gamma函数表达为： $$\frac{\Gamma(n + 1)}{\Gamma(n - k + 1)} x^{n - k}$$ k可以表示为分数，即可求得分数阶导数。 Bohr-Mullerup 定理若$f: (0,\infty) \rightarrow (0,\infty)$ 且满足： $f(1) = 1$ $f(x + 1) = xf(x)$ $\log {f(x)}$ 是凸函数 那么$f(x) = \Gamma(x)$ Digamma 函数$$\Psi(x) = \frac{d \log{\Gamma(x)}}{dx}$$ Digamma函数在涉及求Dirichlet分布相关的参数的极大似然估计时，往往要用到。 Digamma函数具有以下性质： $$\Psi(x + 1) = \Psi(x) + \frac{1}{x}$$ Gamma 分布对gamma函数的定义做一个变形： $$\int _0^{\infty} \frac{x^{\alpha - 1} e^{-x}}{\Gamma(\alpha)}dx = 1$$ 将积分部分视为概率密度，得到Gamma分布密度函数 $$Gamma(x|\alpha) = \frac{x^{\alpha - 1} e^{-x}}{\Gamma(\alpha)}$$ $x = \beta t$得到一个一般形式： $$Gamma(t|\alpha, \beta) = \frac{\beta^{\alpha}t^{\alpha - 1} e^{-\beta x}}{\Gamma(\alpha)}$$ 在实际应用中， 指数分布和$\chi^2$分布都是特殊的Gamma分布 Dirichlet 分布如果说要用一句话来简要介绍Dirichlet分布的话，那就是分布之上的分布。下面从Beta分布来引入对Dirichlet分布的介绍。 Beta 分布用一个比较实际的情况来解释beta分布： $X_1,X_2,…,X_n \sim Uniform(0,1)$ 把这n个随机变量排序，$X_{(k)}$的分布式Beta分布 如何确定？ 通过计算$X_{k}$落在一个区间$[x, x + \Delta x]$的概率，其中$[0,x)$有 k-1 个数， $(x,1]$有n-k 个数 $$\begin{eqnarray} \nonumberP(E) &amp; = &amp; \prod_{i = 1}^{n} P(X_i) \\ \nonumber&amp; = &amp; x^{k-1} (1 - x - \Delta x)^{n - k} \Delta x \\&amp; = &amp; x^{k - 1}(1 - x)^{n - k}\Delta x + o(\Delta x) \nonumber\end{eqnarray}$$ $$\begin{eqnarray} \nonumberP(x \leq X_{k} \leq x + \Delta x) &amp; = &amp; n \begin{pmatrix}n - 1 \\ k -1 \end{pmatrix} P(E) + o(\Delta x) \\ \nonumber&amp; = &amp; n \begin{pmatrix}n - 1 \\ k -1 \end{pmatrix} x^{k - 1} (1-x)^{n - k}\Delta x + o(\Delta x) \nonumber\end{eqnarray}$$ $X_{k}$的概率密度函数为$$\begin{eqnarray} \nonumberf(x) &amp; = &amp; \lim_{\Delta x \rightarrow 0}\frac{P(x \leq X_{k} \leq x + \Delta x)}{\Delta x} \\ \nonumber&amp; = &amp; n \begin{pmatrix}n - 1 \\ k -1 \end{pmatrix} x^{k - 1} (1-x)^{n - k}\\&amp; = &amp; \frac{n !}{(k-1)!(n-k)!} x^{k - 1} (1-x)^{n - k} \nonumber\end{eqnarray}$$ 利用gamma函数，可以表示为 $$f(x) =\frac{\Gamma(n+1)}{\Gamma(k)\Gamma(n-k+1)} x^{k - 1} (1-x)^{n - k}$$ 取$\alpha = k$, $\beta = n - k + 1$ $$f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta )} x^{\alpha - 1}(1- x)^{\beta - 1}$$ 即是beta分布。 Beta-Binomial 共轭用一个比较实际的情况来解释Beta-Binomial共轭： $X_1,X_2,…,X_n \sim Uniform(0,1)$ ，排序后对应顺序统计量$X_{(1)}, X_{(2)}, … ， X_{(n)} $我们要猜测 $p = X_{(k)}$ $Y_1, Y_2, …, Y_m \sim Uniform(0,1), Y_i$中有$m_1$个比p小，$m_2$个比p大 问$P(p|Y_1, Y2, …, Y_m)$ 的分布是什么。 根据beta分布，上面情况中的 1 可以推导出 p 的分布为Beta分布 $f(p) = Beta(p|k, n - k + 1)$，也称为p的先验分布 根据上述中的 2， 相当于是做了m次伯努利实验，所以 $ m_1 $服从二项分布 在给定了来自数据提供的 $(m_1 , m_2)$知识后，p的后验分布变为 $f(p|m_1, m_2) = Beta(p|k+m_1, n - k + 1 + m_2)$ 根据贝叶斯参数估计的基本过程： $$先验分布 + 数据知识 = 后验分布$$ 以上贝叶斯分析的简单直观的表述就是 $$Beta(p|k, n - k + 1) + BinomCount(m_1, m_2) = Beta(p|k+m_1, n - k + 1 + m_2)$$ 所以Beta-Binomial共轭式为： $$Beta(p|\alpha, \beta) + BinomCount(m_1,m_2) = Beta(p|\alpha + m_1, \beta + m_2)$$ 此处共轭的意思就是数据符合二项分布的时候，参数的先验分布和后验分布都能保持Beta分布的形式。 Dirichlet - Multinomial 共轭首先我们需要先了解下什么是Dirichlet分布 用一个比较实际的情况来解释Dirichlet分布： $X_1,X_2,…,X_n \sim Uniform(0,1)$ 排序后对应顺序统计量$X_{(1)}, X_{(2)}, … ， X_{(n)} $ 问$(X_{(k_1)}, X_{(k_1 + k_2)})$ 的联合分布是什么。 通过类似Beta分布的推导，我们可以得到联合分布为：$$\begin{eqnarray} \nonumberf(x_1,x_2,x_3) &amp; = &amp; \frac{n!}{(k_1 - 1)!(k_2 - 1)!(n - k_1 - k_2)!}x_1^{k_1 - 1}x_2^{k_2 - 1}x_3^{n - k_1 - k_2} \\ \nonumber&amp; = &amp; \frac{\Gamma (n + 1)}{\Gamma (k_1)\Gamma (k_2)\Gamma (n - k_1 - k_2 + 1)}x_1^{k_1 - 1}x_2^{k_2 - 1}x_3^{n - k_1 - k_2} \\ \nonumber\end{eqnarray}$$ 该分布就是3维形式的Dirichlet分布，令$\alpha_1 = k_1, \alpha_2 = k_2, \alpha_3 = n - k_! - k_2 + 1$，概率密度可以写成： $$f(x_1,x_2,x_3) = \frac{\Gamma (\alpha_1 + \alpha_2 + \alpha_3)}{\Gamma (\alpha_1)\Gamma (\alpha_2)\Gamma (\alpha_3)}x_1^{\alpha_1 - 1}x_2^{\alpha_2 - 1}x_3^{\alpha_3 - 1}$$ Dirichlet分布，其参数是两个标量：维数K和参数向量各维均值$\alpha = \frac{\sum \alpha_l}{K}$ 其分布律的一般形式为： $$p(p^{\rightarrow} | \alpha, K) = Dir(p^{\rightarrow} | \alpha, K) = \frac{\Gamma(K \alpha)}{\Gamma(\alpha)^K} \prod_{k = 1}^K p_k^{\alpha - 1} = \frac{1}{\Delta_k(\alpha)} \prod_{k = 1}^K p_k^{\alpha - 1}$$ $$\Delta_k(\alpha) = \frac{\Gamma(\alpha)^K}{\Gamma(K\alpha)}$$ 那么什么是Dirichlet - Multinomial 共轭呢？ 用一个比较实际的情况来解释Dirichlet - Multinomial 共轭： $X_1,X_2,…,X_n \sim Uniform(0,1)$ ， 排序后对应顺序统计量$X_{(1)}, X_{(2)}, … ， X_{(n)} $ 令$p_1 = X_{(k_1)}$， $p_2 = X_{(k_1 + k_2 )}$， $p_3 = 1 - p_1 - p_2 $， 我们要猜测 $p^{\rightarrow} = (p_1, p_2, p_3 )$ $Y_1, Y_2, …, Y_m \sim Uniform(0,1), Y_i$中落到[$0, p_1 $), [$p_1 , p_2 $), [$p_2 , 1$) 三个区间的个数分别为 $m_1, m_2, m_3, m = m_1 + m_2 + m_3$ 问后验分布$P(p^{\rightarrow} | Y_1, Y_2, … , Y_m)$ 的分布是什么。 以上贝叶斯分析的简单直观的表述就是 $$Dir(p^{\rightarrow} | k^{\rightarrow}) + MultCount(m^{\rightarrow }) = Dir(p^{\rightarrow} | k^{\rightarrow} + m^{\rightarrow})$$ 描述的就是Dirichlet - Multinomial 共轭 下一节将开始介绍LDA模型]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[浅谈主题模型（一） —— 主题模型进化史]]></title>
      <url>%2F2017%2F03%2F28%2F%E6%B5%85%E8%B0%88%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
      <content type="text"><![CDATA[本科阶段，我用NLP的方法处理从网上健康社区爬取下来的问答数据，通过主题模型的构建实现了对患者提问的答案以及专家的智能化匹配过程。本文及以后的一系列“浅谈主题模型”主要谈谈主题模型的历史，着重介绍下LDA模型，同时也对自己曾经的项目做一个总结概括。 在自然语言处理中，一个非常重要的步骤就是word embedding，即将词转为向量的过程。而主题模型在本质上也是一种将词转为向量的方法。本块主要介绍主题模型的发展历程并分析各个模型的优劣之处。 TF-IDF早在上世纪 80 年代，研究者们常常使用 TF-IDF 法来进行文档的检索和信息的提取。 在 TF-IDF 方法中，先选择一个基于词或者词组的基本词汇，然后对于语料库中的每个文档，分别统计其中每个单词的出现次数。经过合适的正则化后，再将词的频率计数和反向文档频率计数相比较，得到一个词数乘以文档数的向量 X。向量的每个行中都包含着语料库中每个文档的 TF-IDF 值。 IF 和 IDF 的概念见下：$$tf_{i,j}=\frac {n_{i,j}}{\sum_k n_{k,j}}$$其中$if_{i,j}$是该词在文件$d_j$中出现的次数，分母表示文件$d_j$中所有字词的出现次数之和。 $$idf_i =\log \frac {|D|}{|\{j:t_i \in d_j\}|}$$ |D|是语料库中的文件总数$|\{j:t_i \in d_j\}|$ 表示包含词语 $t_i$ 的文件数目。如果词语不在语料库中，为了避免分母为零的情况，一般会加上1. $$tfidf_{i,j} = tf_{i,j} \times idf_{i,j}$$ 对于某一特定文件中的高频词语，如果含有该词语的文档在整个文件集中是低频的，则我们可以得到一个较大的TF-IDF的值。因此，TF-IDF倾向于过滤去常见的词语，保留重要的词语。 但是整个TF-IDF算法是建立在一个假设之上的：一个单词出现的文本频数越小，它区别不同类别文本的能力就越大。这个假设很多时候是不正确的，尤其是在引入IDF的过程中，单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用，显然这并不是完全正确的。其也不能有效地反映单词的重要程度和特征词的分布情况，因此精度有限。 LSA 隐形语言分析如果说TF-IDF算法体现的是文档中词出现的频率的情况，那么LSA则更进了一步，其目的是为了从文本中发现隐含的topic。 在文档的空间向量模型中，文档被表示成由特征词出现概率组成的多维向量，这种方法可以通过对不同词赋予不同的权重，应用于文本检索、分类以及聚类等问题中。然而这种空间向量模型没有能力处理一词多义以及一义多词这类问题。同义词也分别被表示成独立的一维，计算向量的余弦相似度时会低估用户期望的相似度，而某个词项有多个词义时，始终对应同一维度，因此计算的结果会高估用户期望的相似度。 而LSA的方法就减轻了类似的问题。LSA使用矩阵的奇异值分解来确定一个在 TF-IDF 特征空间中的线性子空间，实现大幅压缩以及对同义和一词多义等基本语言概念的捕捉。 通过SVD分解，我们可以构造一个原始向量矩阵的一个低秩逼近矩阵，具体的做法是将词项文档矩阵做SVD分解：$$C = U\Sigma V^T$$其中C是以词为行，文档为列的矩阵，设一共有t行d列, 矩阵的元素为词项的TF-IDF值。然后把 $\Sigma$ 的r个对角元素的前k个保留（最大的k个保留）, 后面最小的r-k个奇异值置0, 得到 $\Sigma_k$ ；最后计算一个近似的分解矩阵$$C_k = U\Sigma_kV^T$$ $C_k$ 在最小二乘意义下是C的最佳逼近。由于 $\Sigma_k$ 最多包含k个非零元素，所以 $C_k$ 的秩不超过k。通过在SVD分解近似，我们将原始的向量转化成一个低维隐含语义空间，起到了特征降维的作用。每个奇异值对应的是每个“语义”维度的权重，将不太重要的权重置为0，只保留最重要的维度信息，去掉一些信息 “nosie”，因而可以得到文档的一种更优表示形式。 上图是原始矩阵的SVD分解，下图是只保留权重最大2维，将原始矩阵降到2维后的情况。 pLSA 潜在语义索引概率模型尽管基于SVD的LSA取得了一定的成功，但是其缺乏严谨的数理统计基础，而且SVD分解非常耗时。Hofmann在SIGIR’99上提出了基于概率统计的PLSA模型，并且用EM算法学习模型参数。PLSA的概率图模型如下 其中D代表文档，Z代表隐含类别或者主题，W为观察到的单词，$P(d_i)$表示单词出现在文档的概率，$P(z_k|d_i) $表示文档$d_i$中出现主题$z_k$下的单词的概率，$P(w_j|z_k)$给定主题$z_k$出现单词$w_j$的概率。并且每个主题在所有词项上服从Multinomial分布，每个文档在所有主题上服从Multinomial 分布。整个文档的生成过程是这样的：(1) 以$P(d_i)$的概率选中文档$d_i$；(2) 以$P(z_k|d_i)$的概率选中主题$z_k$；(3) 以$P(w_j|z_k)$的概率产生一个单词$w_j$。我们可以观察到的数据就是$(d_i, w_j)$对，而$z_k$是隐含变量。$(d_i, w_j)$的联合分布为: $$P(d_i,w_j) = P(d_i)P(w_j|d_i), P(w_j|d_i) = \sum_{k = 1}^{K}P(w_j|z_k)P(z_k|d_i)$$ 而$P(z_k|d_i)$和 $P(w_j|z_k)$分别对应了两组Multinomial分布，我们需要估计这两组分布的参数，一般是用EM算法来估算出pLSA的参数。关于EM算法可以详见从EM算法到GMM模型一文。在此不做推导证明，只列出最终答案：$$P(w_j|z_k) = \frac{\sum_{i = 1}^{N}n(d_i,w_j)P(z_k|d_i,wj)}{\sum_{m = 1}^{M}{\sum_{i = 1}^{N}n(d_i,w_j)P(z_k|d_i,w_m)}}$$ $$P(z_k|d_i) = \frac{\sum_{j = 1}^{M}n(d_i,w_j)P(z_k|d_i,wj)}{n(d_i)}$$ 如此不断在E-step和M-step中迭代，直到满足终止条件。（具体的参考过程可以详见这篇博文） pLSA将文档中的每个词建模为一个来自混合模型的样本。这个混合模型中的成分是可以被看作“主题”的多项式随机变量。因此每个词都是由单一一个主题产生的，文档中不同的词语可以由不同的主题产生的。每篇文档可以表示为各种主题按照一定比例的混合，成为主题集下的分布。尽管 Hofmann 的成果对于主题概率模型是具有启发性的，但是在 pLSA 模型中，Hofman 仅仅将文档——主题、主题——词的分布视为参数而非随机变量。这使得模型中的参数数目和语料库呈线性关系，最终会导致由于语料库的增大导致过拟合，此外其也缺乏对训练集以外的文档的理论支持。 LDA (Latent Dirichlet Allocation)David Blei在2003年提出了LDA（Latent Dirichlet Allocation）的概念，对pLSA模型进行了贝叶斯拓展，利用一种层级贝叶斯模型构建了LDA模型，通过把模型的参数看作随机变量，引入控制参数的参数，实现模型概率化，避免同pLSA那样随着语料库的增大而出现过拟合现象。 下下节中会对LDA进行简要的介绍。 自从LDA的概念提出以来，主题模型已经在诸多文本挖掘的领域取得了令人瞩目的成果。主题概率模型不同于以往的空间向量模型（以TF-IDF为例）和语言模型（n-gram 等），它通过主题在词上的概率分布将主题引入文档中，再将文档视为主题的概率分布，从而分析出文档内潜在的主题。主题概率模型的优越性不仅仅体现在其能够分析出文档中的潜在主题，更在于通过主题概率模型，我们能够显著地降低文档特征的维度。相比较于TFIDF中词→文档，主题概率模型通过词→主题→文档将词与文档隔离开，由于主题个数远远小于词数，实现降维的过程。这种显著的降维可以使得针对大数据的分析操作有了实践基础，拥有了更低的训练模型成本。同时更低的维度也将数据自身的噪声影响降得更低，使得训练的结果更加优秀。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Bag of Words for Image Classification]]></title>
      <url>%2F2017%2F03%2F22%2FBag-of-Words-for-Image-Classification%2F</url>
      <content type="text"><![CDATA[最近了解到了一种很有意思的关于图像分类的算法，用bag of words的思想来对图像进行处理，很巧妙地将原本是图像处理的问题变换成了自然语言处理的问题。本文将简要对这种方法进行介绍。 Bag of Words (BoW) 词袋模型原理Bag of words model (BoW model) 最早出现在NLP和IR领域。该模型将一段文字视为一系列无序的单词，忽略掉文本的语法和语序。近年来，BoW模型被广泛应用于计算机视觉中。与应用于文本的BoW类比，图像的特征被当作单词。 在BoW中，我们统计的是一个词在一份文档中出现的次数，比如： 12(1) John likes to watch movies. Mary likes movies too.(2) John also likes to watch football games. 中有10个不同的词，分别可以被构建为一个如下的向量： 12(1) [1, 2, 1, 1, 2, 0, 0, 0, 1, 1] (2) [1, 1, 1, 1, 0, 1, 1, 1, 0, 0] 在邮件过滤中，这种方法有着很好地应用。 在浅谈主题模型中，我所介绍的TF-IDF在本质上也是一种BoW模型，不同在于加上了Term Weighting。 提取图像特征机器学习的第一步也是最重要的一步就是提取特征，这关乎到随后一系列工作中数据的质量问题。在BoW处理图像的过程中，我们可以用Filter Bank或是SIFT等方法来提取 Filter Bank用不同的滤波器对图像进行滤波，得到一系列滤波处理后的图片，随机选取一些pixel，组成一列，然后将所有滤波后的图片一起组成一个矩阵。矩阵中每个元素的值是rgb三值。以此构成的矩阵中的每一行是一个特征向量。 本文下述的相关操作都是基于filter bank的方法来做的。 SIFTSIFT特征虽然也能描述一幅图像，但是每个SIFT矢量都是128维的，而且一幅图像通常都包含成百上千个SIFT矢量，在进行相似度计算时，这个计算量是非常大的，通行的做法是用聚类算法对这些矢量数据进行聚类，然后用聚类中的一个簇代表BoW中的一个视觉词，将同一幅图像的SIFT矢量映射到视觉词序列生成码本，这样每一幅图像只用一个码本矢量来描述，这样计算相似度时效率就大大提高了。 在BoW中我们对每一幅图像提取SIFT特征（每一幅图像提取多少个SIFT特征不定）。每一个SIFT特征用一个128维的描述子矢量表示。每一个SIFT特征即为特征向量。 构建词库创建完特征向量后，我们需要将特征向量和词对应起来（map to words）。 我们可以用k-means的方法对特征向量进行聚类，用欧氏距离表示cluster和特征点之间的距离，通过多次迭代得到的每个cluster就是词。因此我们在进行k-mean的时候要仔细确定好词的数量，太多的词会导致过拟合而太少的词会出现欠拟合现象。 完成这一步后，我们就构建完成了一个词库，接下来就要在图像中表现出这些词。 下图为一些出现的高频“词” 图像中词的直方图分布（Visual Words）对于图像中的每个点，用欧式距离法判断属于哪个词，从而得到图像中词的一个直方图分布。下图表示的是visual word的信息。 为了不完全抛弃图中pixel的位置，我们还可以采用spacial pyramid matching的方法来保留一部分spacial information（表现在所取的每个patch中） 此外，我们甚至可以将TF-IDF的思想引入其中，得到具体的值，在test的过程中通过内积来做相似性度量。 KNN进行图像分类（testing）将training set中的每个直方图分布视为一个vector，先获取testing set中的一幅图的直方图分布，比较和training set中图的距离，找出最近的k个点中所对应training image的对应分类即是该图的分类结果。 需要注意的一点是，k的取值不能太大，尤其是当某一类中的图片较少时更是如此。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Deep Learning Chapter 2]]></title>
      <url>%2F2017%2F03%2F09%2FDeep-Learning-Chapter-2%2F</url>
      <content type="text"><![CDATA[在Deep Learning一书的第二章，主要介绍了线性代数的基本思想。 基本概念标量：单独的数 向量：一列数 矩阵：二维数组 张量：多维数组 在深度学习中，我们有时允许矩阵和向量相加，产生另一个矩阵：$\mathbf{C} = \mathbf{A} + \mathbf{b} $，其中 $C_{i,j} = A_{i,j} + b_j$， 即向量b和矩阵A的每一行相加。 矩阵中元素对应的乘积表示为： $\mathbf{A \odot B}$ 同时需要注意的一点是，只有非奇异的方阵才有逆。所谓非奇异也就是说，方阵的所有列向量都是线性无关的。一个列向量线性相关的方阵被称为奇异的。 范数$L^p$ 范数的定义如下：$$\left | \mathbf{x} \right |_p = (\sum_i{|x_i|^p})^{\frac{1}{p}}$$ 范数是将向量映射到非负值的函数。向量x的范数是衡量从原点到点x的距离。其需要满足下列三个条件： $f(\mathbf{x}) = 0 \Rightarrow \mathbf{x} = \mathbf{0}$ $f(\mathbf{x} + \mathbf{y}) \geq f(\mathbf{x}) + f(\mathbf{y})$ (三角不等式) $ \forall \alpha \in \mathbb{R}, f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}) $ p = 2时，$L^2$被称为欧几里得范数，表示的是原点和x之间的欧氏距离。 平方$L^2$范数： $\mathbf{x}^T x$。 平方$L^2$范数较之$L^2$范数更加的方便，因为前者的导数只和对应元素相关，而后者的导数却是和整个向量相关。 但是有时我们仍然会遇到在原点附近增长缓慢的情况，对于一些区分零和非零小值的机器学习应用而言是十分致命的。因此很多时候会采用$L^1$范数，每当x中的某一元素增加了一个小量时，对应的防暑也会增加这个小量。 深度学习中，我们有时为了衡量矩阵的大小，使用类似$L^2$范数的Frobenius范数： $$\left | \mathbf{A} \right |_F = \sqrt{\sum_i{A_{i,j}^2}}$$ 正交矩阵正交矩阵是指行向量和列向量都是标准正交的方阵：$$\mathbf{A}^T \mathbf{A} = \mathbf{A} \mathbf{A}^T = \mathbf{I}$$即：$$\mathbf{A}^{-1} = \mathbf{A}^T$$ 由于正交矩阵的求逆计算代价很小，所以在机器学习中经常得到运用 特征分解所谓特征分解就是讲矩阵分解为一组特征向量和特征值。方阵A的特征向量是指与A相乘后等于对该向量进行缩放的非零向量v：（我们一般只考虑单位特征向量）$$\mathbf{A v} = \lambda \mathbf{v}$$其中的标量λ被称为这个特征向量对应的特征值。 假设矩阵A有n个线性无关的特征向量$\{\mathbf{v^1},…,\mathbf{v^n}\}$，对应着特征值$\{\lambda_1,…\lambda_n\}$，我们将特征向量连接为一个矩阵，使得每一列是一个特征向量：$\mathbf{V} = \{\mathbf{v^1},…,\mathbf{v^n}\}$，将特征向量连接成一个向量$\mathbf{\lambda} = \{\lambda_1,…\lambda_n\}^T$A的特征分解：$$\mathbf{A} = \mathbf{V} diag(\mathbf{\lambda})\mathbf{V}^{-1}$$ 但是，不是每一个矩阵都可以分解为特征值和特征向量的。只有实对称矩阵才可以分解为实特征向量和实特征值，但是分解结果可能不唯一。如果两个或多个特征向量拥有相同的特征值，那么这组特征向量生成子空间中，任意一组正交向量都是该特征值对应的特征向量。$$\mathbf{A} = \mathbf{Q} \Lambda \mathbf{Q}^{T}$$上式中Q是A的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。因为Q是正交矩阵，我们可以将A看作是沿方向$\mathbf{v}^{i}$延展$\lambda_i$倍的空间 奇异值分解 SVD奇异值分解也是一种分解矩阵的方法，将矩阵分解为奇异向量和奇异值，可以得到一些类似特征分解的信息。 SVD的应用更广，对于没有特征分解的非方阵矩阵，只能使用奇异值分解。 $$\mathbf{A} = \mathbf{U D V}^T$$ U和V都是正交矩阵，D是对角矩阵。 矩阵U的列向量是左奇异向量，是$\mathbf{A} \mathbf{A}^T$的特征向量矩阵V的列向量是右奇异向量，是$\mathbf{A}^T \mathbf{A}$的特征向量 SVD主要用于拓展矩阵求逆到非方矩阵上。 Moore-Penrose 伪逆非方矩阵的逆矩阵没有定义，对于下列的问题我们希望可以用矩阵A的左逆B来求解线性方程。 $$\mathbf{A x} = \mathbf{y}$$ $$\mathbf{x} = \mathbf{B y}$$ 如果矩阵A的行数大于列数，那么上述方程可能没有解。如果矩阵A的行数小于列数，那么上述矩阵可能有多个解。 Moore-Penrose 伪逆定义：$$\mathbf{A}^+ = \lim_{a \rightarrow 0}(\mathbf{A}^T \mathbf{A} + \alpha \mathbf{I})^{-1} \mathbf{A}^T$$ $$\mathbf{A}^+ = \mathbf{V} \mathbf{D}^+ \mathbf{U}^T$$ 其中矩阵U、D、V是由A奇异值分解后得到的矩阵，对角矩阵D的伪逆$\mathbf{D}^+$是其所有非零元素取导数之后再转置得到的。 PCA 应用]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从EM算法到GMM模型]]></title>
      <url>%2F2017%2F03%2F08%2F%E4%BB%8EEM%E7%AE%97%E6%B3%95%E5%88%B0GMM%E6%A8%A1%E5%9E%8B%2F</url>
      <content type="text"><![CDATA[最近温习了下机器学习经典算法之一的EM算法，顺便对GMM模型有了更加深入的理解，本文将对这两个概念进行简要介绍。 最大似然估计（MLE）Maximum Likelihood Estimation 在介绍EM算法之前，不得不提最大似然估计。 最大似然估计可以解释为： 假设 $ p (x | \omega) $ 是一个有参数向量 $\theta $ 唯一确定的概率分布 参数 $\theta $ 是固定但是未知的。 假设我们有一个按照概率分布 $ p (x | \omega) $ 的数据集 D，D 中的样本彼此独立。 MLE 就是 $\theta $ 的一个最能解释描述该数据集的值。 我们也可以通俗地解释为： 最大似然估计一般用于求分布参数 $\theta $ : 给定一个概率分布，我们从概率分布中抽n个值的采样，通过这些采样数据来估计概率分布的参数 $\theta $，定义似然函数如下所示： $$\begin{eqnarray}lik(\theta ) &amp; = &amp; f_{D}(x_{1},x_{2},…,x_{n} | \theta ) \\&amp; = &amp; \prod_{k = 1}^{N}p(x_k| \theta ))\end{eqnarray}$$ $$\theta’ = \arg \max_{\theta }\{p(Data|\theta )\}$$ 在 $\theta $ 的所以取值上令一阶导数等于0，使得这个函数取到最大值，这个使可能性最大的 $ \theta ‘ $ 值即为 $\theta $ 的最大似然估计。 最大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们就把这个参数作为估计的真实值。 那么我们要如何来解决ML estimate的问题呢？ 我们先设 $\mathbf{\theta }$ 为一个p元素的向量 $\mathbf{\theta } = [\theta_1,\theta_2,…,\theta_p]^T$ 让其成为梯度算子 $ \bigtriangledown_\theta =[\frac{\partial }{\partial \theta_1}, \frac{\partial }{\partial \theta_2},…, \frac{\partial }{\partial \theta_p} ] $ 已有：$ p (x | \mathbf{\theta}) = \prod_{k=1}^{n}p(x_k | \theta)) $ 我们定义 $ l(\mathbf{\theta})$ 为最大似然函数,以及对应的梯度算子：$$l(\mathbf{\theta}) = log(p(D |\mathbf{\theta})) = \sum_{k = 1}^{n}log(p(x_k | \mathbf{\theta}))$$$$\bigtriangledown_\theta l(\mathbf{\theta}) =\bigtriangledown_\theta log(p(D |\mathbf{\theta})) = \sum_{k = 1}^{n}\bigtriangledown_\theta log(p(x_k | \mathbf{\theta}))$$ 我们通过算子为0得到最大似然估计：$$\bigtriangledown_\theta l(\mathbf{\theta}) = 0$$ ML主要应用于单高斯和多高斯中。 EM算法Expectation-Maximum Algorithm EM算法一般用于在概率模型中找最大似然估计，主要针对含有潜在变量的，将不完全的数据补成完全的。 对于含有潜在变量的，如果我们还是按照求最大似然估计的方法来解决的话（分别求偏导），会出现“和的对数”这种难以解决的情况。对于这种情况，我们要用辅助函数来帮助解决。 我们定义一个辅助函数 $ A(x,x^t)$ 与 f(x) 在 $x^t$ 处相等，且满足 $f(x) \geq A(x,x^t) $ 我们只要将辅助函数的最大值设为新的 $\theta$ ，通过多次迭代逐渐逼近 f(x) 的最大值。 我们可以利用Jensen不等式来推导： 根据Jensen不等式，我们可以得到，如果f是凸函数，x是随机变量，则： $$E[f(x)] \geq f(E[x])$$ 如果是凹函数则反之 因此我们可以推导得出一个辅助函数：(将潜在变量引入其中，log函数是凹函数) $$\begin{eqnarray}\log{p(\mathbf{X} | \Theta)} &amp; = &amp; \log{\sum_y{p(\mathbf{x},y | \Theta)}} \\&amp; = &amp; \log{q(y)\frac{\sum_y{p(\mathbf{x},y | \Theta)}}{q(y)}} \\&amp; \geq &amp; \sum_y q(y) \log(\frac{p(\mathbf{x},y | \Theta)}{q(y)})\end{eqnarray}$$ 因此，EM算法具体可以分为以下四个步骤： 对参数进行初始化设置，为 $\Theta^{old}$ E-step: 估计出 $p(y | x, \Theta^{old})$$$Q(\Theta, \Theta^{old}) = \sum_y{p(y | x, \Theta^{old}) \log(p(\mathbf{x},y|\Theta))}$$ M-step: 用最大似然估计来估计$ \Theta^{new}$$$\Theta^{new} = \arg \max_\Theta Q(\Theta, \Theta^{old})$$ 检查是否收敛，如果不是$\Theta^{new} \rightarrow \Theta^{old}$ 并返回步骤2 EM 模型主要用于高斯混合模型 GMM 中。 GMM模型在一个多高斯混合模型中，数据是从多个单高斯中生成出来的，即：$$P(x) = \sum_{k=1}^{K} \pi_k N(x;u_k,\Sigma_k)$$其中，$\pi_k$ 是权重值，总的和为1. 对于一个GMM模型，如果K的值足够大的话，混合模型就可以逼近任意的连续概率分布。 GMM模型本质上是一种聚类算法，每个高斯分布就是一个聚类中心。和K-means不同的在于，GMM是采用概率模型来表达聚类原型，因此可以给出一个样本属于某类的概率大小。 在每个样本的分类确定的情况下，GMM的参数可以直接用MLE的方法来确定。但是在只知道样本点而不知道其分类情况下，我们会得到一个这样的log-likehood的函数：$$\sum_{i = 1}^N \log(\sum_{k = 1}^K \pi_k N(x;u_k,\Sigma_k))$$在这个似然函数中，由于每个样本$x_i$所属的类别$z_k$是属于隐含变量的未知值，所以我们需要用EM算法来计算出模型的参数（使得上式的期望最大），然后可以用这些算好的参数来对样本进行分类。 E-step: 假设模型参数已知，求隐含变量分类Z的期望，即个个高斯分布的概率 M-step: 根据E-step中求得的分类，用MLE的方法求得参数。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[做一只孤独的美食家]]></title>
      <url>%2F2017%2F02%2F23%2F%E5%81%9A%E4%B8%80%E5%8F%AA%E5%AD%A4%E7%8B%AC%E7%9A%84%E7%BE%8E%E9%A3%9F%E5%AE%B6%2F</url>
      <content type="text"><![CDATA[本篇随笔记录了一次意外的美食之行，在广州能尝到美味的怀石料理，简直是不能更满足。幸甚至哉，遂作此篇，见笑。 如果问我，生命中什么是重要的？两年前的我大概会回答说唯美食与美女不可弃，而现在的我的回答就要简练的多 —— 唯美食不可弃。看吧，不变的始终是那份对美食的热爱。 作为吃货的我有两部珍藏已久的电视剧，每当情绪低落或是想要静下心来的时候就会打开欣赏，感觉整个人一下子便获得了宁静，一种潜藏着的小喜悦变会逐渐地洋溢开来，暖却全身。这两部便是《深夜食堂》与《孤独的美食家》，一部是从家常的食物间找寻人生的真谛，一部是在平凡的生活中寻觅非凡的美味。美食即人生，两者间的交错互融才使得我们的生活更加丰富多彩。 由于要急着要简历照，我不得不在下午三点赶到珠江新城进行拍摄。在前一天预约的时候，我就心想这时间不尴不尬好生难受。拍完会学校吧，岂不枉费一次进城放荡的机会；拍完吃饭吧，又离着饭点有些遥远。 仔细这么一琢磨，我倒也觉得是该好好放松下身心了，所以就找起了周边的美食。一边找着，心里就浮现出了《孤独的美食家》中，五郎每每办完正事后在街头巷尾找寻美味的画面，以及那句在三个层层拉近的镜头后说出的“好饿啊！”。这么想着想着就怀念起了在日本吃过的那次怀石料理（脑洞好大。。。），恰逢enjoy上有一家店正搞着优惠，机不可失我便决定拍完照后前去享受一番。 说了这么多的废话，换做高中的语文阅读，这些便叫做引出主题。 写了这么多心理的执念和脑洞，此时再犹抱琵琶半遮面岂不扫兴？话不多说直接上图： 一进入餐厅落座，一封日文写的小纸条早已躺在桌台上，虽然看不太懂，但是心里总还是暖暖的。 首先端上来的就是作为吸物的时令和风汤，是由南瓜等熬制而成，入口醇厚，香甜而不腻，微咸而素雅。在今日寒风凛冽的广州可真是驱寒开胃的佳品。 由于餐厅内客人较多，服务员和我商议后决定先将第四道菜品，烧物，端上来先行品尝。这次的烧物由银鳕鱼烹制而成，配上脆爽可口的秋葵，筋道美味的菌菇，以及银鳕鱼的油而不腻，饱满而又丰腴的鱼肉一次次融化在我的舌尖，日料的美味真是让人赞不绝口。 接下来的也是我最为喜欢的，就是八寸和刺身了。这盘菜端上来的时候，服务员还特地为我一一介绍，从品种到做法到吃饭，也让我见识了一番。分别有鲜甜嫩滑的蓝鳍金枪鱼刺身，肉质厚重味道鲜美的稠鱼刺身，还有甜虾、青鱼和海螺刺身。此外还有美味的和牛寿司、土豆沙拉球、玉子烧、春卷、腌制番茄以及入口爆裂回味无穷的三文鱼籽。好一番大快朵颐！ 接下来就是香甜的和牛肉炖土豆以及鲜美的汤了 而压轴的寿司也是让我不虚此行，尤其是蓝鳍金枪鱼寿司，简直好次到炸裂！鳗鱼寿司也是软糯美味到不行！ 最后的抹茶布丁也是让我口留余香，抹茶的厚重与布丁的甜美交织在一起，升华了这次的用餐。 如果可以，做一名孤独的美食家可好？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Deep Learning Chapter 1]]></title>
      <url>%2F2017%2F02%2F22%2FDeep-learning-Chapter-1%2F</url>
      <content type="text"><![CDATA[最近想要系统地学习下deep learning，因此找了Ian Goodfellow写的”Deep Learning”一书来啃，我会将每章所讲内容整理后放在blog上。本篇就简要介绍下deep learning一书的第一章，Introduction。 What is Deep Learning？什么是深度学习？虽然这一词早已为大家所熟知，但是这一概念可能大部分人都不胜了解。其实一张简单的韦恩图就可以清晰简洁的表示： 简单地说，深度学习是一种表示学习(representation learning)，也是一种机器学习，可用于许多AI方法。 那么深度学习又和之前的这些概念有什么区别呢？ 从这张图中我们可以看出，对于经典的机器学习方法，我们需要手动设计出特点进行导入来实现目的。而Representation Learning则是能够自己发掘特征，找到并导入来实现结果。 而对于深度学习而言，特征的找取是分多层多次实现的：先是通过原有的数据进行简单的特征提取，再根据提取的简单特征，进一步提取出更加抽象的特征来，这层递增，最终实现结果。 以图像识别的例子而言就是这样的： 第一层可以是edge，通过图像中像素值的突变来实现edge这一特征的提取； 第二层可以是corners and contours，在edge的基础上，我们可以识别出轮廓和角的边的集合，将轮廓和角等提取出来； 第三次则是object parts，通过轮廓和角的集合，我们可以测定处特定对象的整个部分； 有了上述这三个hidden layer，我们就可以最终判断出图像是哪种物体了。 Brief Introduction about the History之前我上过Ng在coursera上的“Machine Learning”课，在课的后半部分简要地带过了一些关于神经网络的知识点。如今想来，二三年前深度学习还大部分时候还是以神经网络这一概念展现在我们面前的。 的确，深度学习作为一个早已提出的概念，经历过三次发展浪潮，直到最近才以 Deep Learning 这一词示人。 在Ng课中介绍的神经网络，就是一种从神经科学的角度出发的简单的线性模型。这些模型被设计为使用一组n个输入{x1,…,xn}，并将其与一个输出y关联，这些模型希望学习一组权重{w1,…,wn},输出y=x1 w1 + … + xn wn，多层这样的结果就构成了这种早期的神经网络的雏形。一般是用back-propagation的方法来进行参数的训练。这种方法直至今日仍然是训练深度模型的主导方法。 由于多层的结构再加上多层之间的信息传递（训练时的正向、反向传播），使得这一模型也被称为神经网络模型。的确神经科学被视为深度学习研究的一个重要灵感来源，但是由于我们没有足够的关于大脑的信息来进行进一步研究指导，其作用逐渐被削弱。现在的大多数神经网络是基于一个成为Rectified linear unit的神经单元模型。 在80年代，神经网络在认知科学的背景下随着Connectionism或者说parallel distributed processing的出现而引来了又一个发展高潮。通过将大量简单的计算单元连接在一起构成网络从而实现智能的行为，用分布式表示的方法，将多个特征彼此分立开，每个输入都有多个特征表示，每个特征都参与表示多个输入，从而分立进行训练。随着back-propagation的普及，这种方法走向了潮流。 随着硬件的发展，当年遥不可及的计算代价如今得以了实现，从而引来了第三次的发展。而大量的数据集的出现也使得训练变得更为方便和准确，而模型规模的增大也使得能够实现更多复杂的内容。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Zero padding]]></title>
      <url>%2F2017%2F02%2F21%2FZero-padding%2F</url>
      <content type="text"><![CDATA[本文详细介绍在信号处理过程中的zero padding的作用和意义。 相关概念CTFT连续时间傅里叶变换(Continuous time Fourier Transform),公式在此不做赘述，表示的是连续无限长的时间域函数通过傅里叶变换转换到连续无限长的频域函数。CTFT的概念存在于物理世界中，在真正现实中处理信号时是无法得到连续信号的，必然是离散的 DTFT离散时间傅里叶变换(Discrete time Fourier Transform)，表示的是离散无限长的时间域函数通过傅里叶变换转换到连续无限长的频域函数。同样的DTFT的概念存在于信号处理的理论中，在真实处理信号时是无法得到无限长的离散采样点，必然是有限长的离散采样点。 DFT离散傅里叶变换(Discrete Fourier Transform)，表示的是有限长的离散时间域函数通过傅里叶变换转换到离散有限长的频域函数。这是我们在信号处理时采用的方法。需要注意的是，在运用DFT的时候还要注意香农定理，避免出现混叠 DFT存在的问题在实际处理信号的过程中，我们发现：由于时域采样点的个数有限，频域上的频率反映会和实际的有一定差别，如下图所示 无限长的时域函数的傅里叶变换结果： 有限长的时域函数的傅里叶变换结果 我们发现不仅会出现side lope，还会出现△f。 解决方法最简单的解决方法就是增加采样点的个数，然后在实际中由于硬件等一系列的原因，我们无法实现增加采样点，这时就可以用zero padding的方法来进行补救。 zero paddingzero padding本质上是通过在不影响结果的情况下，通过增加采样点的个数来实现对插值结果的预测，从而更好地表示变换后的细节。 时域上的 zero padding时域上的zero padding主要是在采样点后增加一定长度的值为0的点。 增加后的结果中，我们可以很明显的看到，△f明显有了减小。 频域上的 zero padding对于频域而言，zero padding之前我们要先了解频域的分布，对于一个频域函数而言，一半的位置是正负值的分水岭，而其表示直流分量的零点则表示在一头一尾处，所以我们在进行zero padding的过程中，就要先将频域函数对半切开，在中间增加为0的点。只有这样才会在不影响原有的频域的分布基础上，增加频域的范围。 我们知道，对于DFT和IDFT而言，频域点的个数和时域点的个数是相同的，所以我们在这样做之后，就相当于拓展了时域的点的个数，在原有两点中间加上了插值点，表现出了细节。 zero padding的意义不改变数据无论是频域和时域上的zero padding，都不会改变数据本身，仅仅只是改变了数据样本点的密度。 更好地体现细节无论是频域和时域上的zero padding，都增加了数据样本点的密度，等价于在对应的另一个域上进行了插值。而这个插值我们原本是无从计算的，正是通过zero padding才实现了对插值的估计。 便于应用FFT算法我们知道，快速傅里叶变换FFT算法需要 2 的 n 次方的样本点，而通过zero padding可以将样本点的个数凑成 2 的 n 次方，便于实现算法上的优化。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[我看日剧]]></title>
      <url>%2F2017%2F02%2F08%2F%E6%88%91%E7%9C%8B%E6%97%A5%E5%89%A7%2F</url>
      <content type="text"><![CDATA[关于夏天的记忆，是浴衣，花火，以及长泽雅美。而关于日剧的记忆，我想，除了帅哥美女，就是对人性的入木三分，以及，对生活的不离不弃。 大概是被《逃避虽可耻但有用》中可爱的gakki所吸引，整个寒假我都沉浸在日剧的汪洋中，从最开始“单纯”欣赏各色女主的颜，到一步步被日剧的精彩攥着一刻不想脱身。 我猜想，大部分热爱日剧的人，应该和我一样，都是通过新垣结衣、长泽雅美、山下智久或是山田孝之等盛世美颜第一次与日剧结缘。不同于某半岛国千篇一律的妖艳美丽，风流倜傥，岛国演员们给人最深的印象就是各具风骚，可以是gakki治愈人心的回眸一笑，可以是长泽大妈令人痴迷的美腿，可以是户田惠梨香调皮的兔牙······不过，自古美人如名将，不教人间见白头，仅凭颜值的话，注定是无法长久的。那么又是什么给我们以长久的追剧动力呢？ 作为一个比较理性的观众，近年来的国产剧和韩剧令我望而却步的主要原因，一是对爱情的过分追求，达到无处不在的程度。不论是刑侦还是医疗剧，主线一定是主角的爱情生活，为爱痴狂，为爱奉献，为爱牺牲。的确我不能否认，爱情是人生中重要的一环，但是生活并不仅仅只有爱情，这种将其剥离然后高高贡起的做法着实令我厌恶。 而另一主因，就是剧的深度问题。对比日剧，国产剧和韩剧更像是流水化工厂的结晶，一模一样的套路配上似曾相识的表演，完成对爱情的讴歌和才子配佳人的美好结局。作为一部青春剧，《求婚大作战》就能让我感慨“花有重开时，人无再少年”，人生需要抓住每一个当下，这样才能在回首往昔时不留遗憾，无需一次次的“哈利路亚chance”。作为一部医疗剧，《code blue》能让我在了解直升机救援的同时深入医护人员的内心，体验日式的医患关系，治人需治心。作为脑洞剧的开山怪，《父女七日变》带我们走进了青春期父女的内心世界，感触温润亲情的同时也为编剧的想象力所折服。更别提《legal high》《半泽直树》这类直击社会的写实之作，更是用坚如磐石的三观圈住了大片的饭。 从这一部部十集左右的剧中，我们所看到的不单单是丰富的情感，精彩的表演，跌宕的情节，更有着对人性的挖掘和讨论，对生活的感触和憧憬，我想这或许就是日剧在剥离一位位盛世美颜后仍能深攥人心的原因吧。 或许，在未来的某一天，我也会抛弃日剧，但是，日剧曾带给我的那份感动，那份对生活的憧憬，将在我内心留下烙印，永久不褪。 【写在最后】有人说看韩剧找不到男朋友，看日剧找不到女朋友。我希望这不是真的。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[巨人的陨落]]></title>
      <url>%2F2017%2F01%2F18%2F%E3%80%8A%E5%B7%A8%E4%BA%BA%E7%9A%84%E9%99%A8%E8%90%BD%E3%80%8B%2F</url>
      <content type="text"><![CDATA[都说历史是一个任人打扮的小姑娘，可架不住看客犀利的目光。从没有一段“淡妆浓抹总相宜”，浓妆艳抹和略施粉黛间的取舍全仗笔者的底蕴。 抽空拜读了肯·福莱特世纪三部曲之一的《巨人的陨落》，深有感触。其通过POV方式的以小见大，带我领略了一战前后的英德法俄美。从大国外交到战争，从权贵达官到平民百姓，书中从一位位主人公的视角出发，勾勒出了一段段充满感情的鲜活历史。 简介全书以4个国家中的5个家族为线索（俄国的别斯科夫、英国的菲兹赫伯特和威廉姆斯、德国的冯·乌尔里希、美国的杜瓦），通过第一人称POV手法，详细介绍了各自国家在一战前后的各种状况。 作者巧妙地将这5个家族定位在不同的阶级上，既有传统贵族的菲兹赫伯特与冯乌尔里希家族，也有新兴的杜瓦家族，同时还有工人阶级的威廉姆斯家族以及代表着传统俄国农民在变革时代间变迁的别斯科夫家族。 正是这巧妙的定位，使得我们得以如亲历般见证着： 一战爆发前英德法俄奥之间的外交对话。个人对这段格外着迷，以一个个国家精英的视角生动展现了国家政治外交上的隐忍与取舍。 一战中贵族的军官视角，平民的战士视角。正是一战加速了欧洲大陆传统贵族的没落。 俄国二月革命、十月革命的萌芽与诞生（列宁的归来） 战后协约国间形如分赃般的丑恶嘴脸，以及战后德国的糟糕境地。 本书以 Fall of Giants 为名，以一 fall 表示了欧洲大陆经过一战后的由盛转衰，同时还意味着战后的欧洲传统贵族开始走向了没落，战时战后尤以英国为主的妇女平权运动打破了以往男盛女衰的格局。 最令我感到惊讶的是作者写在文后的那句话 我的原则是：要么某一场景真实发生过，或者有可能发生；要么某些话真正说过，或者有可能说。如果我发现有某种原因让某种场景不可能真正发生，或不可能说出某些话——例如某个人物当时处于另一个国家，我便将其略去。 我想，这或许就是读完全文如此舒畅的原因了吧。 Thoughts如果说要让我从整本书中选出最有感触的一段，我想就要数关于俄国革命的部分了。 书中一段格雷戈里的内心活动一下子就抓住了我的心 一个孩子的成长就像一场革命，格雷戈里心想，你可以让他诞生，但后来如何就全然不在你的掌控之下了。 不像我国媒体般吹嘘十月（二月）革命的伟大，也不同西方媒体般对苏维埃敬而远之，书中以及其克制的手法，分别从俄国国内贵族和平民，以及英国贵族与平民的角度，多方面阐述了对革命的看法。 有沙皇治下的市民吃不上面包，有警察枪击手无寸铁的游行群众，也有革命之时大街上的打砸抢烧；有贵族不经审判绞死“侵占”自家空闲领地的农民，有军官贪污军需置士兵的死活与不顾，也有革命成功后苏维埃政权向贫穷农户强征口粮甚至枪毙不配合者。 的确，革命势在必行，但是在打破旧制度制定新制度的过程中，又有谁能确保自己不会成为那些打败恶龙最终成为恶龙的勇士呢？苏维埃是少数敢于站出来挑战沙皇权威的政党，以反对对人民的压迫，还人民以土地作为党的纲领，但是执政后也强抢粮食并枪毙不肯交粮的农民，甚至枪毙倒向孟什维克党的工人。 或许恶龙和勇士就是一条莫比乌斯带，我们永远不知道究竟是那一面。 写在文后：上述感想只是单纯讨论苏维埃政权，请勿发散思维！！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Head First Civilization V]]></title>
      <url>%2F2017%2F01%2F09%2FCivilization-V%2F</url>
      <content type="text"><![CDATA[开篇让我们将地球的时钟拨到远古世纪，蜿蜒的河流边正孕育着文明的曙光，一队队移民正取下行装，准备在历史的长河中书写壮丽的篇章。一座座简陋的城市如雨后春笋般拔地而起，勾勒着美好的希望。手持木棒的勇士正开疆拓土，怀揣着一统天下的梦想。 上述这些画面描述的就是文明系列游戏的开场。一把文明游戏就仿佛是人类文明盛衰兴亡的缩影，带给我的感触绝非是一般游戏可比拟的。一句话，文明系列，不仅仅是游戏。 如果说有什么游戏值得我整整一天宅在电脑前废寝忘食，那一定是席大大的文明系列。而在6代bug不断，4代画质感人的今日，5代的美丽新世界显得格外诱人，在一个又一个“next turn”的指引下，一切都如白驹过隙，一晃而逝。（友情提示，没有闹钟的，没有自制力的，请远离文明系列） 宏观看文明虽然不像P社四萌那样有着极高的历史还原度，但是在宏观层面上对人类文明的量化和物化我从未见过如文明这般卓越的。它将人类文明的核心量化为科技和文化，而各个国家的实力物化为一个个城市以及单位，彼此之间相互影响，构成了整个复杂交错的文明世界，也让玩家看到了数千年来人类文明发展的缩影，闲暇之余颇有感触。 科技与文化在Civilization这系列游戏中，人类文明时刻被文化和科技点数推进着前行。随着科技的飞速发展，科技树茁壮成长着，一个个闻所未闻的建筑和单位出现在了城市的建设列表中，等待着一国之主的召唤。它们将进一步促进科技和文化的发展。而随着文化的积累，一项项政策被从上锁的黑箱拿上了台面，如同现在的城市规划和国家方针一样，指引着帝国步入黄金时代，迈入胜利的殿堂。 的确，人类的文明从来都是一个厚积而薄发的过程，没有前期挖石头的技术，哪来之后的石油、煤矿等的大发现？没有律法的出现，哪来的意识形态的选择？纵使给远古时代的人民一百个爱因斯坦，也注定不会有相对论和量子力学的丁点萌芽。这也是为什么，在游戏中，政策的选择局限于时代，科技的研发取决于之前的积累。 城市与单位玩家通过建造城市设施和调整市民工作来实现一个城市在粮食、产能、金钱、文化和信仰等方面的平衡，使之为帝国的发展贡献力量。每每在进入城市界面时，我的脑海中浮现的不是一个个单调的数字或是乏味的图标，而是一个个活生生的市民为了生计在城市之郊辛勤工作，为城市的产能和产粮做出微不足道的贡献。这样的生化当然难言快乐，尤其是当城市人口急剧膨胀之际，辛勤劳作了一天的市民不得不蜷曲在狭小的住处休息，这也是为什么在游戏中人口和城市数量的增长会带来大幅的不快乐。于国家而言，哪里有城市，哪里就有剥削，除非危及自身统治，人民的快乐不足为道，这又何尝不是现实生活的一个缩影呢？通过产出奢侈品和建设娱乐设施，市民们的快乐可以得到大幅提升，似乎被剥削的不快早已被抛之脑后，看来娱乐至死的年代由来已久啊。 如果说用一句话总结文明系列的精髓，那就是国家的利益高于一切。在游戏界面中，渺渺众生不过是一串串简单的数字，一队队忠义无双的士兵，一个个任劳任怨的劳工。他们在玩家的带领下，为着最终的胜利默默奉献着自己。游戏中，为了保住珍贵的笑脸，我们往往不惜将攻占的城市付之一炬，看着城市的人口一回合一回合地下降，直至城池变为废墟。为了省下足够的金钱，我们往往会将多余的单位就地处决，看着他们化作一道光永远消逝在历史的长河中。为了抢在AI之前造出奇观，我们往往会强制让市民忍饥挨饿，没日没夜地为产能贡献力量。沉浸在游戏中的我们又怎会意识到呢，这些不就是曾经的“嘉定三屠”，“扬州十日”，“长平之战”后的白起杀降，布尔什维克在战后依然实施的“战时共产主义”？ 唯一带点个人色彩的也就是在城市中产出的伟人了，他们有着与真实历史中某位名人相同的名字，而在游戏中他们的命运也不外乎是为了一项奇观、一项科技、一个作品或是一场战役，鞠躬尽瘁死而后已。可即便如此，他们还是一个个义无反顾地诞生于城市，为了帝国的繁荣昌盛前赴后继。真可谓是“苟利国家生死以，岂因祸福避趋之”。 国家和城邦文明系列游戏经久不息的一个原因，就在于多样的文明种类。玩家可以从二三十个文明中挑选一个进行游戏，而这二三十个文明也都各具特色。好玩的一点在于，游戏选取这些文明中的一位著名统治者，将其作为游戏中该文明的领袖。所以在游戏界面中，我们得以看这样的场景，骑在马上的拿破仑不屑地评论着我羸弱的士兵，白宫中的华盛顿义正言辞地进行抗议，雍容华贵的武则天不怒自威地否决了我的交易。正是这些文明领袖们将我们更好地带入了游戏中，将复杂难表的国家关系用领袖们的喜怒哀乐鲜活地呈现给了玩家。 在文明游戏中，国家关系是一个很重要的部分。两国间的良好关系是进行合作研究以及现金交易的前提。同时两国间的贸易也必须在非战争状态的条件下才能进行。这不就是现实的一个缩影吗？ 除了国家之外，在游戏中玩家还会碰到许多城邦，也就是现实中的“小国”。在第一次遇到时它们会给你一些钱，我总是开玩笑地说这是供奉给大国的礼金，摆出低调的姿态祈求乱世中的苟存。刚玩的时候我总是对这些城邦不屑一顾，对这些蝼蚁一直提出的烦人任务不理不睬。看着它们一个个成为了其他国家的盟友，想着这些小国送给你们也没什么用处，就这样一直开开心心地在家种田。一转眼国际形势骤变，昔日笑脸相迎的对手最终兵戎详见，我惊讶地发现那些城邦也都紧跟着它们主子的步伐，争先恐后地对我宣战。我当时第一个想到的不是打开存档，而是“这不就是北约？”。玩得越多，我越觉得城邦好处大大的，盟友会给你兵，给你文化，给你粮食，给你资源，给你信仰，还能给你快乐！而如果觉得完成城邦任务太麻烦，大可以拿钱砸，一堆堆金子砸在城邦的脸上，玩家就能享受到如同城邦爸爸一般的待遇，给吃给喝还能帮打架。 可以说文明中对国家和城邦的设定很好地诠释出了大国与小国间的外交政策与外交姿态。 胜利种类在Civilization V这款游戏中，总共有5种胜利的方式，而这5种方式也给玩家以不同的感悟。 外交胜利有人说，联合国建立的意义是为了让全世界更好地按照大国意志来运转，而事实也确有几分相似。诸如伊拉克战争、古巴禁运等事件就是大国通过对联合国的控制来实现对小国的支配，从而更好地实现自身利益。在文明游戏中也是亦然，一旦世界议会选出了世界领袖，那么也就意味着该国有着将自己的意志以世界议会中某一议题的形式强加于他国的能力，也就实现了外交上的胜利。 征服胜利文明V这款游戏也被称为野蛮V，不打仗怎么赢天下。一个无论科技和文化都远远落后的文明要想赢得游戏，只有用武力打破现有的状态。一旦取得了所有文明的首都，就取得了征服胜利。 作为一名爱好和平的种田流玩家，从个人理念出发我是十分不喜欢玩征服胜利的，一个原因自然是因为操纵“百万大军”攻城下寨是一件极其繁琐的事。还有一个很重要的原因是我在游戏过程中探索出来的：在飞船起飞前的一回合，我抱着玩一玩的心态将我的“小男孩一号”原子弹投向了身边的埃及，瞬间一座人口30+的主城血条只剩下了一半左右，周边的单位也是死伤惨重，城市中隐约还有人群大喊哭泣的声音，虽然知道这不过是游戏一场，但是内心总感觉有些堵。当然啦，打仗是一定要打的，不过我更喜欢逼得对面割地赔款，然后我在安安心心发战争财。 科技胜利从仰望星空开始的不仅仅有哲学，还有科学！将人类文明传播到更为浩瀚的宇宙一直是科学家们的梦想，也支撑着他们燃烧自己，成就科学的跃进。当飞船起飞的一刹那，人类文明的种子已播撒在银河系中，承载着无数人的梦想驶向远方。这也是我最喜欢的胜利模式！ 文化胜利当玩家的文化支配了其他文明，当那里的人民都穿着我们的衣服，说着我们的语言的时候，我们还能说这是别的国家吗？文化胜利的本质就是通过本国文化对他国潜移默化的影响，通过人民的融合实现国家的兼并。实现难度着实不小（一般都是打出来的。。。） 时间胜利（一般都关闭时间胜利）满满都是吐槽。。。在世界末日前的那一刻，上帝说，拯救那个分数最高的文明吧，于是就有了时间胜利。。。（2050年分数最高的玩家获胜） 结尾开了程序猿的脑洞写下了这篇civilization的Head First，文明带给我的不仅仅是绝佳的游戏体验，还让我有了很多思考，打算有时间出一篇Thinking in Civilization。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Concurrent Programming]]></title>
      <url>%2F2016%2F12%2F10%2Fconcurrent-programming%2F</url>
      <content type="text"><![CDATA[并发在应用程序中起着很重要的作用，本文将详细介绍应用级的并发 进程进程是构造并发程序最为简单的方法。在CSAPP的第八章中就已经介绍过：进程本质上就是一个执行中的程序的实例，每当我们运行一个程序的时候就会创建一个进程并在其上运行相关文件。而进程在真正运行的过程中并不是独占处理器的，根据不同的逻辑控制流（每个进程的PC值），不同的进程轮流使用处理器。如果不同的进程在运行过程中有时间的重叠，则两者之间是并发的关系。采用进程的并发方式可以在父子进程之间共享文件，同时两者不同的地址可以避免彼此信息的覆盖问题。但是这种方式不得不采用IPC（进程间通信）的方式来交换彼此的信息，而这是一种开销很大的方式，大大降低运行的速度。 I/O多路复用当我们在浏览一个网页的时候，服务器可以同时处理浏览器发送的请求和用户输入的指令，而这主要采用的就是I/O多路复用的方法。其核心思想就是采用select函数，要求内核挂起进程，仅当一个或多个I/O事件发生后才将其返回给应用程序。本质上这种方法下我们创建自己的逻辑流，利用I/O多路复用来进行流的调度。这种方法的一个最大的优点就是信息交换的便捷，共享数据来得更为高效(无需在流之间切换)，使我们对程序有着更好的掌控但是与此同时，与第一种方法相比，编码量的复杂度大大提升。而且一旦某一逻辑流在读某一文本，其他流就不能读了。这也是不是很高效的一点。 线程什么是线程与进程是运行在系统中的逻辑流对应的，线程是运行在进程中的逻辑流，每个线程都有着唯一的整数ID、栈指针、栈、计数器、寄存器等等，运行在一个进程中的线程共享该进程的整个虚拟地址空间。从本质上讲，这种方法更像是上述两种方法的结合。 线程是如何执行的所有的进程在最开始的时候都是单线程的，这个线程就是主线程。随后在某一时间点主线程会创建一个对等线程并与之一起并发运行（来回切换）由于线程的context对比进程而言要小得多，所以线程之间的切换也要快得多。主线程和对等线程之间基本上是相同的，都能读写相同的共享信息。 线程相关函数创建线程1234567#include &lt;pthread.h&gt; typedef void *(func)(void *);// Returns 0 if OK, nonzero on error int pthread_create(pthread_t *tid, pthread_attr_t *attr, func *f, void *arg); // Returns thread ID of callerpthread_t pthread_self(void); 结束线程当调用下述函数时，主线程会等待所有对等线程终止时在终止自己和整个进程。否则则当线程运行完后隐式终止。 123// terminate threads#include &lt;pthread.h&gt; void pthread_exit(void *thread_return); 分离线程在线程被创建后，其默认是可结合的，即可以被其他线程回收杀死，而下面的函数则可以将其分离，仅当其终止时才自动释放存储。 123// detach threads#include &lt;pthread.h&gt; int pthread_detach(pthread_t tid); // Returns 0 if OK, nonzero on error 线程中同步变量各个线程彼此之间可以共享变量和文件，但是如果不加限制有时会造成同步错误。因此，在文件或是变量同步(读写)的过程中，并发的程序有着种种的限制。在本书讲pipeline的过程中就介绍过read after write的问题。在pipeline中如果先写后读则读的过程至少需要等待三个周期才能保证不出错（当然在forwarding的方法下我们可以将等待周期减为1个）。同样的，我们在并发线程中进行文件或是变量读写操作的时候，也会遇到类似的问题：如果在某一线程读取某一变量值的同时，另一线程正在对改写这一变量(这里的同时指的并不是完全意义上的同时，而是很短的时间)，由于读和写都要一定时间，这就可能会造成数据的错误。因此我们需要对线程间的变量同步加以限制。主要采用Posix中的 P 和 V 操作。 P(s)：加锁操作。若s非零则将其减1返回，否则挂起线程直至s非零。 V(s)：解锁操作。若有线程被P操作挂起则将s加1，重启该线程。 因此，我们可以通过 P 和 V 操作实现线程中的变量同步。以下代码展示了读者优先的线程，只要有一个读者在读，其他的读者就能忽略锁而毫无障碍的读取变量。 1234567891011121314151617181920212223// global variablesint readcnt;sem_t mutex, w;void reader(void)&#123; while(1)&#123; P(&amp;mutex); readcnt++; if (readcnt == 1) &#123; P(&amp;w); &#125; V(&amp;mutex); // do the reading P(&amp;mutex); if (readcnt == 0) &#123; V(&amp;w); &#125; V(&amp;mutex); &#125;&#125; 线程中的竞争问题如果我们在构建线程时，每次创建一个新的对等线程都是通过传递一个指向唯一整数ID的指针的话，很有可能会导致程序的错误，因为在这种情况下各个线程会产生竞争。而解决这种问题的方法也很简单，只需要用一个malloc函数为每个线程动态分配一个整数ID的指针，并将这个这个指针传递给构建线程的函数中。同时最后别忘了对指针进行free来避免memory leak。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[proxy-lab]]></title>
      <url>%2F2016%2F12%2F09%2Fproxy-lab%2F</url>
      <content type="text"><![CDATA[Proxy Lab 作为 cmu 18600 以及 15213 这两门课的最后一个lab，其综合性非常强。既需要掌握好 web programming 以及 concurrent programming 的相关知识，还需要结合之前涉及的 shell 和 cache 的相关操作。本文将详细介绍 proxy lab 的解题思路。 什么是proxy首先，我们需要知道什么是proxy？当我们平时打开浏览器的时候，输入一个URL，浏览器会向服务器发送相应的请求，服务器在接收到请求后会将相应的response发回给浏览器，如此循环往复从而加载完网页中的全部内容。此时所有的请求和响应之间的交流全是发生在 client (浏览器)和 server (服务器)之间的。而有时我们会在client和server之间添加代理，来进行相关的处理，这个代理就是实验要求我们完成的proxy。 proxy的大致示意图如下所示 实验准备在本次实验中，我们采用的浏览器是Firefox，设置代理的过程如下所示：打开设置中的高级，选择网络，点设置并按照如下设置（若proxy在本地则选择localhost或是127.0.0.1）需要注意的是，端口一定要和之后运行proxy时的端口一致 proxy如何处理request打开Firefox网页，Mac下alt + cmd + q，Win下按F12进行观察，点击每条可以显示出请求和响应的内容 通过这种方法，我们可以很轻松地看到请求和响应头。 而作为一个proxy，所需要做的事情主要有这么几件： 从请求中获取请求的方法，请求网址的hostname，path以及port（没有的话为80）(有多种方法解析，我采用的是正则表达式) 需要注意的是，本实验中不支持非get的方法，同时也不支持任何以HTTPS开头的网页请求。本实验中以501错误返回这类请求。 改变请求头中的一些内容（比如User-Agent，connection改为close） 添加 Proxy-Connection: close，来确定请求响应的交换是否结束 改变原先请求中的version。（从 HTTP/1.1 到HTTP/1.0） 对网页端发送请求进行修改之后，发送给服务器，再将response返回给网页端，如此循环往复直到网页内容加载完毕 处理多线程操作在完成上述内容之后，我们就实现了一个逐条处理网页端请求的proxy。但是在现实中这样的效率极其低下。所以对于我们的proxy还需要使其支持多线程操作。其所涉及的函数如下所示 1234int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine) (void *), void *arg);int pthread_detach(pthread_t thread); 其中pthread_create函数用于打开一个新的线程，通过调用start_routine这个函数，而其中的arg是start_routine函数的参数。特别要注意的是，arg一定要事先进行malloc，为每个线程ID分配一个独立的块，并将指向这个块的指针传给start_routine。不然线程会出现竞争问题导致错误。同时在结束线程时一定要释放这些块来避免memory leak（我就是在这里跪了很久的。。。） 而第二个函数是用于在线程中防止线程被其他线程回收或杀死。 在第一步中加入上述函数，基本上就能够实现proxy的多线程操作的部分。 存储网页内容以上的proxy已经基本完成了代理的要求。不过当我们重复请求某一个网址的时候，它还是要重新加载一遍，这就有点低效了。如果我们能够把之前网页端获取的响应存下来呢？这样当我们重复加载的时候就无需连接到服务器了。所以，我们还要让我们的proxy能够存储网页的内容。在本次实验中，我用一个类似于队列的双向链表来表示存储的cache。proxy在每次处理完网页端发来的请求后，先遍历整个链表，看是否有相同的request存在cache中，如果有就直接获取对应的response。没有的话就现将请求发动到server，将server返回的response写入到cache中。在具体的操作中，我采用的是FIFO，每次都将新的request/response加在链表的头。一旦cache存储已满，就从尾部pop。需要注意的是，一旦找到匹配的request之后，我们还需要将对应的node移到链表的头指针处。这样才符合FIFO。 上述就是我对与proxy lab的总结，希望大家都能做出一个完美的proxy！！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[去雾算法浅析]]></title>
      <url>%2F2016%2F11%2F29%2F%E5%8E%BB%E9%9B%BE%E7%AE%97%E6%B3%95%E6%B5%85%E6%9E%90%2F</url>
      <content type="text"><![CDATA[何凯明博士在09年以 “Single Image Haze Removal Using Dark Channel Prior” 一文技惊四座。此文甚至成为09年的CVPR最佳论文。本篇博文将对这种去雾算法进行简要的分析，并通过自己的实现来更好的进行介绍。 算法核心介绍Foggy Image model首先，我们需要构建一个雾天图像的模型。公式如下所示，其中I表示雾天图像，J表示没有雾的图像，t表示空气穿透率，A表示纯雾。式子中的x均表示为图像中的像素点。 上式非常容易理解：我们最终看到的雾天图像是由透过空气传递过来的原始景观图像和一定浓度的雾叠加而成。 而作为去雾，我们是要依据已有的雾天图像I来得到原始的景观图像J，这就需要利用暗通道原理来求得模型中的空气穿透率t。 暗通道基于观察，何凯明博士提出了这样的假设：自然界中的大部分非天空物体，其rgb三个颜色通道中至少有一个值非常低，其表达式如下所示： 其中Ω为x*x的像素方块。 下图是先取图像中三个通道的最小值构成一张灰度图，然后在做一个一定窗口大小的最小值滤波，最终得到暗通道处理图。 通过处理图片我们可以清晰看出，除了白色的部分（三通道的值都比较大），其余部分的暗通道值很小，近似符合暗通道趋向0的假设。 去雾模型推导对于雾天模型的公式，我们现在做如下处理： 为了得到最终的去雾图像，我们需要求得t和A。推导过程如下所示： 由于即便在正常的天气中，原始景观传过来的时候也是经过了一定浓度的大气（可以认为是雾）。而对于参数A，我们是这样计算的：选取一定区域内亮度最大的值作为A（理论上认为纯雾接近纯白）这样就将t和A的值分别求了出来，从而得到去雾后的图像J。 采用这样的方法得到的图像如下所示： 由于在图像处理的过程中都不是基于一个像素而是一个像素集合的方块进行的，所以实际得到的图像中物体边缘会有比较明显的白边的存在，而对于这种现象，何凯明博士先后提出了两种理论予以解决。 去除伪影Soft Matting 2009该方法较为复杂，且处理较慢，早已不用，本文不做赘述 Guided Filter 导向滤波 2011导向滤波先做了一个如下的先验假设：所有邻近的点之间都是线性的，同时任何方程在局部的小段都可以认为是线性的。 经过推导可得： 其中q为输出图像，I为输入图像，p为滤波图像。i和k都是像素index，a和b是线性参数。在导向滤波中，I可以为任何导向图像 上述式子的一大优点就是其可以保护边界值。 通过上述式子我们可知，当像素强度变化不大时，a趋向于0而b等同于窗口的均值强度。等同于做一个均值滤波。 而当强度变化大时，a趋向于1而b趋向于0。等同于不做滤波，保留边界。 对之前的两张图片进行导向滤波处理，我们可以得到较好的结果。 总结最后我将展示一些去雾的效果： 去雾算法在处理非天空的自然物体时效果很好。如图： 同时在处理城市建筑时也有着不俗的表现。如图： 但是同时其也有着一些不足之处 比如在处理以白色物体为背景的图像时，会出现去雾效果不好以及色彩偏蓝的现象。这主要是因为去雾算法是基于暗通道值趋向为0的先验假设，而白色背景是不符合这种先验假设的，由于不符合物理模型所以才导致出错。如图： 此外当雾太厚的时候，还会出现过曝的情况，这是由于穿透参数t很小，根据之前的推导，我们会得到一个较大的J(x)。这才导致了过曝的现象。如图： 所以总而言之，去雾算法可以取得一个比较好的效果，不过也有其一定的局限性，要视具体的情况而定。 想要上手参考的，详见我的github]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[First Article]]></title>
      <url>%2F2016%2F11%2F23%2FFirst-Article%2F</url>
      <content type="text"><![CDATA[浮生偷得几日闲，遂作此blog，望与有志者相识于此。 What will be in this blog技术文作为一名计算机专业的学生，开此blog的初衷自然是强化学习的动力，促进自己去研究前沿的知识，并以博文的形式与大家共享。 随笔“意皆有所郁结，不得通其道，故述往事,思来者”，自古这就是知识分子的通病，想来我也不能例外。在本blog中，我也会时不时将自己的所想所感放上来，权当一种情绪宣泄的途径，让各位见笑。 游记二十余载间也游历了不少大好河山，附上游记既可为诸位提供行程指导，同时也为了加深这些美好的印象。 趣谈Why so serious!? Let’s have some fun. What won’t be in this blog负能量正如我们使用Python的理由——“life is short”，何苦牢骚满腹呢。虽说是情绪的宣泄，也只是抒发人生的思考，给出自己的见解，绝非自怨自艾，叹命途多舛，哀人生不公。]]></content>
    </entry>

    
  
  
</search>
